{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 100,
  "global_step": 2346,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0256,
      "grad_norm": 0.12366539239883423,
      "learning_rate": 0.00019838022165387896,
      "loss": 2.3907,
      "step": 20
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.10239621996879578,
      "learning_rate": 0.0001966751918158568,
      "loss": 2.2421,
      "step": 40
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.11474200338125229,
      "learning_rate": 0.0001949701619778346,
      "loss": 2.0857,
      "step": 60
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.09948132932186127,
      "learning_rate": 0.00019326513213981247,
      "loss": 2.0443,
      "step": 80
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.09605875611305237,
      "learning_rate": 0.0001915601023017903,
      "loss": 2.014,
      "step": 100
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.09744413942098618,
      "learning_rate": 0.00018985507246376812,
      "loss": 2.0062,
      "step": 120
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.0948501005768776,
      "learning_rate": 0.00018815004262574597,
      "loss": 1.9617,
      "step": 140
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.09232772886753082,
      "learning_rate": 0.0001864450127877238,
      "loss": 1.9804,
      "step": 160
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.09841462224721909,
      "learning_rate": 0.00018473998294970162,
      "loss": 1.9435,
      "step": 180
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.10508353263139725,
      "learning_rate": 0.00018303495311167947,
      "loss": 1.9241,
      "step": 200
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.10176949203014374,
      "learning_rate": 0.0001813299232736573,
      "loss": 1.9549,
      "step": 220
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.10448456555604935,
      "learning_rate": 0.00017962489343563513,
      "loss": 1.9182,
      "step": 240
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.10182049870491028,
      "learning_rate": 0.00017791986359761298,
      "loss": 1.9329,
      "step": 260
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.10339523106813431,
      "learning_rate": 0.0001762148337595908,
      "loss": 1.9289,
      "step": 280
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.1044064313173294,
      "learning_rate": 0.00017450980392156863,
      "loss": 1.9768,
      "step": 300
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.10879046469926834,
      "learning_rate": 0.00017280477408354648,
      "loss": 1.9338,
      "step": 320
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.10827518999576569,
      "learning_rate": 0.0001710997442455243,
      "loss": 1.978,
      "step": 340
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.10190510749816895,
      "learning_rate": 0.00016939471440750213,
      "loss": 1.9398,
      "step": 360
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.10490282624959946,
      "learning_rate": 0.00016768968456948,
      "loss": 1.9293,
      "step": 380
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.10758119076490402,
      "learning_rate": 0.0001659846547314578,
      "loss": 1.9357,
      "step": 400
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.10030393302440643,
      "learning_rate": 0.00016427962489343564,
      "loss": 1.9318,
      "step": 420
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.10247227549552917,
      "learning_rate": 0.0001625745950554135,
      "loss": 1.9396,
      "step": 440
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.10712835192680359,
      "learning_rate": 0.00016086956521739132,
      "loss": 1.9457,
      "step": 460
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.10969021916389465,
      "learning_rate": 0.00015916453537936914,
      "loss": 1.9145,
      "step": 480
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10454913228750229,
      "learning_rate": 0.000157459505541347,
      "loss": 1.9424,
      "step": 500
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.12666979432106018,
      "learning_rate": 0.00015575447570332482,
      "loss": 1.9617,
      "step": 520
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.10863997042179108,
      "learning_rate": 0.00015404944586530265,
      "loss": 1.9075,
      "step": 540
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.10538257658481598,
      "learning_rate": 0.0001523444160272805,
      "loss": 1.9389,
      "step": 560
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.1036733016371727,
      "learning_rate": 0.00015063938618925833,
      "loss": 1.9167,
      "step": 580
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.10363077372312546,
      "learning_rate": 0.00014893435635123615,
      "loss": 1.9512,
      "step": 600
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.11162691563367844,
      "learning_rate": 0.000147229326513214,
      "loss": 1.92,
      "step": 620
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.11151790618896484,
      "learning_rate": 0.00014552429667519183,
      "loss": 1.918,
      "step": 640
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.10605470836162567,
      "learning_rate": 0.00014381926683716966,
      "loss": 1.936,
      "step": 660
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.11312747001647949,
      "learning_rate": 0.0001421142369991475,
      "loss": 1.9339,
      "step": 680
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.10029371827840805,
      "learning_rate": 0.00014040920716112533,
      "loss": 1.9162,
      "step": 700
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.11216779798269272,
      "learning_rate": 0.00013870417732310316,
      "loss": 1.9245,
      "step": 720
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.11210418492555618,
      "learning_rate": 0.000136999147485081,
      "loss": 1.9089,
      "step": 740
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.10104293376207352,
      "learning_rate": 0.00013529411764705884,
      "loss": 1.8875,
      "step": 760
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.1022961288690567,
      "learning_rate": 0.00013358908780903666,
      "loss": 1.9318,
      "step": 780
    },
    {
      "epoch": 1.02304,
      "grad_norm": 0.1120949238538742,
      "learning_rate": 0.00013188405797101452,
      "loss": 1.9287,
      "step": 800
    },
    {
      "epoch": 1.04864,
      "grad_norm": 0.11130255460739136,
      "learning_rate": 0.00013017902813299234,
      "loss": 1.9289,
      "step": 820
    },
    {
      "epoch": 1.07424,
      "grad_norm": 0.11181901395320892,
      "learning_rate": 0.00012847399829497017,
      "loss": 1.9439,
      "step": 840
    },
    {
      "epoch": 1.09984,
      "grad_norm": 0.11382172256708145,
      "learning_rate": 0.00012676896845694802,
      "loss": 1.9316,
      "step": 860
    },
    {
      "epoch": 1.12544,
      "grad_norm": 0.11251501739025116,
      "learning_rate": 0.00012506393861892585,
      "loss": 1.9611,
      "step": 880
    },
    {
      "epoch": 1.15104,
      "grad_norm": 0.1054808497428894,
      "learning_rate": 0.00012335890878090367,
      "loss": 1.9438,
      "step": 900
    },
    {
      "epoch": 1.17664,
      "grad_norm": 0.11052307486534119,
      "learning_rate": 0.00012165387894288151,
      "loss": 1.9233,
      "step": 920
    },
    {
      "epoch": 1.20224,
      "grad_norm": 0.11699593812227249,
      "learning_rate": 0.00011994884910485935,
      "loss": 1.9516,
      "step": 940
    },
    {
      "epoch": 1.22784,
      "grad_norm": 0.11087436228990555,
      "learning_rate": 0.00011824381926683719,
      "loss": 1.908,
      "step": 960
    },
    {
      "epoch": 1.2534399999999999,
      "grad_norm": 0.1184961199760437,
      "learning_rate": 0.00011653878942881502,
      "loss": 1.9628,
      "step": 980
    },
    {
      "epoch": 1.27904,
      "grad_norm": 0.11981222778558731,
      "learning_rate": 0.00011483375959079286,
      "loss": 1.9265,
      "step": 1000
    },
    {
      "epoch": 1.30464,
      "grad_norm": 0.11719213426113129,
      "learning_rate": 0.00011312872975277067,
      "loss": 1.9259,
      "step": 1020
    },
    {
      "epoch": 1.3302399999999999,
      "grad_norm": 0.11685425788164139,
      "learning_rate": 0.00011142369991474851,
      "loss": 1.9403,
      "step": 1040
    },
    {
      "epoch": 1.35584,
      "grad_norm": 0.1038600280880928,
      "learning_rate": 0.00010971867007672633,
      "loss": 1.9272,
      "step": 1060
    },
    {
      "epoch": 1.38144,
      "grad_norm": 0.11308852583169937,
      "learning_rate": 0.00010801364023870417,
      "loss": 1.8854,
      "step": 1080
    },
    {
      "epoch": 1.40704,
      "grad_norm": 0.11250393092632294,
      "learning_rate": 0.00010630861040068201,
      "loss": 1.912,
      "step": 1100
    },
    {
      "epoch": 1.4326400000000001,
      "grad_norm": 0.11418042331933975,
      "learning_rate": 0.00010460358056265984,
      "loss": 1.891,
      "step": 1120
    },
    {
      "epoch": 1.45824,
      "grad_norm": 0.1063656285405159,
      "learning_rate": 0.00010289855072463768,
      "loss": 1.9066,
      "step": 1140
    },
    {
      "epoch": 1.48384,
      "grad_norm": 0.10910797864198685,
      "learning_rate": 0.00010119352088661552,
      "loss": 1.8984,
      "step": 1160
    },
    {
      "epoch": 1.5094400000000001,
      "grad_norm": 0.12328897416591644,
      "learning_rate": 9.948849104859336e-05,
      "loss": 1.9553,
      "step": 1180
    },
    {
      "epoch": 1.53504,
      "grad_norm": 0.11397156119346619,
      "learning_rate": 9.77834612105712e-05,
      "loss": 1.896,
      "step": 1200
    },
    {
      "epoch": 1.56064,
      "grad_norm": 0.12051747739315033,
      "learning_rate": 9.607843137254903e-05,
      "loss": 1.959,
      "step": 1220
    },
    {
      "epoch": 1.58624,
      "grad_norm": 0.12763464450836182,
      "learning_rate": 9.437340153452686e-05,
      "loss": 1.9082,
      "step": 1240
    },
    {
      "epoch": 1.61184,
      "grad_norm": 0.11353667080402374,
      "learning_rate": 9.26683716965047e-05,
      "loss": 1.9252,
      "step": 1260
    },
    {
      "epoch": 1.63744,
      "grad_norm": 0.11339258402585983,
      "learning_rate": 9.096334185848254e-05,
      "loss": 1.9033,
      "step": 1280
    },
    {
      "epoch": 1.66304,
      "grad_norm": 0.12068620324134827,
      "learning_rate": 8.925831202046036e-05,
      "loss": 1.9456,
      "step": 1300
    },
    {
      "epoch": 1.68864,
      "grad_norm": 0.10894633829593658,
      "learning_rate": 8.75532821824382e-05,
      "loss": 1.9026,
      "step": 1320
    },
    {
      "epoch": 1.71424,
      "grad_norm": 0.12349274754524231,
      "learning_rate": 8.584825234441604e-05,
      "loss": 1.9057,
      "step": 1340
    },
    {
      "epoch": 1.73984,
      "grad_norm": 0.11219487339258194,
      "learning_rate": 8.414322250639387e-05,
      "loss": 1.9088,
      "step": 1360
    },
    {
      "epoch": 1.76544,
      "grad_norm": 0.11315460503101349,
      "learning_rate": 8.243819266837171e-05,
      "loss": 1.898,
      "step": 1380
    },
    {
      "epoch": 1.79104,
      "grad_norm": 0.11657258868217468,
      "learning_rate": 8.073316283034955e-05,
      "loss": 1.8784,
      "step": 1400
    },
    {
      "epoch": 1.81664,
      "grad_norm": 0.11295446008443832,
      "learning_rate": 7.902813299232737e-05,
      "loss": 1.9193,
      "step": 1420
    },
    {
      "epoch": 1.8422399999999999,
      "grad_norm": 0.12527617812156677,
      "learning_rate": 7.73231031543052e-05,
      "loss": 1.8948,
      "step": 1440
    },
    {
      "epoch": 1.86784,
      "grad_norm": 0.1224779561161995,
      "learning_rate": 7.561807331628304e-05,
      "loss": 1.8717,
      "step": 1460
    },
    {
      "epoch": 1.89344,
      "grad_norm": 0.11928901076316833,
      "learning_rate": 7.391304347826086e-05,
      "loss": 1.8691,
      "step": 1480
    },
    {
      "epoch": 1.9190399999999999,
      "grad_norm": 0.11306474357843399,
      "learning_rate": 7.22080136402387e-05,
      "loss": 1.8663,
      "step": 1500
    },
    {
      "epoch": 1.9446400000000001,
      "grad_norm": 0.12886464595794678,
      "learning_rate": 7.050298380221654e-05,
      "loss": 1.9121,
      "step": 1520
    },
    {
      "epoch": 1.97024,
      "grad_norm": 0.11054457724094391,
      "learning_rate": 6.879795396419437e-05,
      "loss": 1.8995,
      "step": 1540
    },
    {
      "epoch": 1.9958399999999998,
      "grad_norm": 0.1183878630399704,
      "learning_rate": 6.70929241261722e-05,
      "loss": 1.9271,
      "step": 1560
    },
    {
      "epoch": 2.02048,
      "grad_norm": 0.11967352777719498,
      "learning_rate": 6.538789428815005e-05,
      "loss": 1.9399,
      "step": 1580
    },
    {
      "epoch": 2.04608,
      "grad_norm": 0.11169988662004471,
      "learning_rate": 6.368286445012787e-05,
      "loss": 1.8959,
      "step": 1600
    },
    {
      "epoch": 2.07168,
      "grad_norm": 0.12622115015983582,
      "learning_rate": 6.197783461210571e-05,
      "loss": 1.8777,
      "step": 1620
    },
    {
      "epoch": 2.09728,
      "grad_norm": 0.1241639107465744,
      "learning_rate": 6.0272804774083543e-05,
      "loss": 1.895,
      "step": 1640
    },
    {
      "epoch": 2.12288,
      "grad_norm": 0.11374383419752121,
      "learning_rate": 5.856777493606138e-05,
      "loss": 1.9102,
      "step": 1660
    },
    {
      "epoch": 2.14848,
      "grad_norm": 0.11034580320119858,
      "learning_rate": 5.6862745098039215e-05,
      "loss": 1.9085,
      "step": 1680
    },
    {
      "epoch": 2.17408,
      "grad_norm": 0.1275305449962616,
      "learning_rate": 5.515771526001705e-05,
      "loss": 1.9099,
      "step": 1700
    },
    {
      "epoch": 2.19968,
      "grad_norm": 0.11942930519580841,
      "learning_rate": 5.345268542199489e-05,
      "loss": 1.9257,
      "step": 1720
    },
    {
      "epoch": 2.22528,
      "grad_norm": 0.1264689415693283,
      "learning_rate": 5.174765558397272e-05,
      "loss": 1.9102,
      "step": 1740
    },
    {
      "epoch": 2.25088,
      "grad_norm": 0.12478946149349213,
      "learning_rate": 5.004262574595056e-05,
      "loss": 1.8864,
      "step": 1760
    },
    {
      "epoch": 2.27648,
      "grad_norm": 0.11947087943553925,
      "learning_rate": 4.833759590792839e-05,
      "loss": 1.9381,
      "step": 1780
    },
    {
      "epoch": 2.30208,
      "grad_norm": 0.11667706817388535,
      "learning_rate": 4.6632566069906224e-05,
      "loss": 1.8951,
      "step": 1800
    },
    {
      "epoch": 2.32768,
      "grad_norm": 0.12082477658987045,
      "learning_rate": 4.492753623188406e-05,
      "loss": 1.9612,
      "step": 1820
    },
    {
      "epoch": 2.35328,
      "grad_norm": 0.11773363500833511,
      "learning_rate": 4.3222506393861896e-05,
      "loss": 1.9192,
      "step": 1840
    },
    {
      "epoch": 2.37888,
      "grad_norm": 0.11684306710958481,
      "learning_rate": 4.151747655583973e-05,
      "loss": 1.9038,
      "step": 1860
    },
    {
      "epoch": 2.40448,
      "grad_norm": 0.11568276584148407,
      "learning_rate": 3.981244671781757e-05,
      "loss": 1.9047,
      "step": 1880
    },
    {
      "epoch": 2.4300800000000002,
      "grad_norm": 0.12199757993221283,
      "learning_rate": 3.81074168797954e-05,
      "loss": 1.8931,
      "step": 1900
    },
    {
      "epoch": 2.45568,
      "grad_norm": 0.11301786452531815,
      "learning_rate": 3.640238704177323e-05,
      "loss": 1.9021,
      "step": 1920
    },
    {
      "epoch": 2.48128,
      "grad_norm": 0.1256706863641739,
      "learning_rate": 3.469735720375107e-05,
      "loss": 1.9134,
      "step": 1940
    },
    {
      "epoch": 2.5068799999999998,
      "grad_norm": 0.13800771534442902,
      "learning_rate": 3.2992327365728904e-05,
      "loss": 1.8956,
      "step": 1960
    },
    {
      "epoch": 2.53248,
      "grad_norm": 0.11997038871049881,
      "learning_rate": 3.128729752770674e-05,
      "loss": 1.9338,
      "step": 1980
    },
    {
      "epoch": 2.55808,
      "grad_norm": 0.12410780042409897,
      "learning_rate": 2.9582267689684573e-05,
      "loss": 1.9117,
      "step": 2000
    },
    {
      "epoch": 2.58368,
      "grad_norm": 0.11905184388160706,
      "learning_rate": 2.787723785166241e-05,
      "loss": 1.879,
      "step": 2020
    },
    {
      "epoch": 2.60928,
      "grad_norm": 0.1201239749789238,
      "learning_rate": 2.617220801364024e-05,
      "loss": 1.9039,
      "step": 2040
    },
    {
      "epoch": 2.63488,
      "grad_norm": 0.11838360130786896,
      "learning_rate": 2.4467178175618073e-05,
      "loss": 1.9087,
      "step": 2060
    },
    {
      "epoch": 2.6604799999999997,
      "grad_norm": 0.11684112250804901,
      "learning_rate": 2.276214833759591e-05,
      "loss": 1.9322,
      "step": 2080
    },
    {
      "epoch": 2.68608,
      "grad_norm": 0.12599191069602966,
      "learning_rate": 2.1057118499573745e-05,
      "loss": 1.9157,
      "step": 2100
    },
    {
      "epoch": 2.71168,
      "grad_norm": 0.12473703175783157,
      "learning_rate": 1.9352088661551578e-05,
      "loss": 1.9431,
      "step": 2120
    },
    {
      "epoch": 2.73728,
      "grad_norm": 0.11950699985027313,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 1.9043,
      "step": 2140
    },
    {
      "epoch": 2.76288,
      "grad_norm": 0.11912189424037933,
      "learning_rate": 1.5942028985507246e-05,
      "loss": 1.89,
      "step": 2160
    },
    {
      "epoch": 2.78848,
      "grad_norm": 0.12038387358188629,
      "learning_rate": 1.423699914748508e-05,
      "loss": 1.953,
      "step": 2180
    },
    {
      "epoch": 2.81408,
      "grad_norm": 0.11769167333841324,
      "learning_rate": 1.2531969309462916e-05,
      "loss": 1.9085,
      "step": 2200
    },
    {
      "epoch": 2.83968,
      "grad_norm": 0.12741807103157043,
      "learning_rate": 1.082693947144075e-05,
      "loss": 1.9385,
      "step": 2220
    },
    {
      "epoch": 2.8652800000000003,
      "grad_norm": 0.1316743791103363,
      "learning_rate": 9.121909633418585e-06,
      "loss": 1.9093,
      "step": 2240
    },
    {
      "epoch": 2.89088,
      "grad_norm": 0.1207604631781578,
      "learning_rate": 7.41687979539642e-06,
      "loss": 1.9119,
      "step": 2260
    },
    {
      "epoch": 2.91648,
      "grad_norm": 0.11001632362604141,
      "learning_rate": 5.711849957374255e-06,
      "loss": 1.9042,
      "step": 2280
    },
    {
      "epoch": 2.94208,
      "grad_norm": 0.12714338302612305,
      "learning_rate": 4.006820119352089e-06,
      "loss": 1.8982,
      "step": 2300
    },
    {
      "epoch": 2.96768,
      "grad_norm": 0.12317749857902527,
      "learning_rate": 2.3017902813299235e-06,
      "loss": 1.8651,
      "step": 2320
    },
    {
      "epoch": 2.99328,
      "grad_norm": 0.13414958119392395,
      "learning_rate": 5.967604433077579e-07,
      "loss": 1.9179,
      "step": 2340
    }
  ],
  "logging_steps": 20,
  "max_steps": 2346,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.779648292487168e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
