{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 5000,
  "global_step": 19535,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0128,
      "grad_norm": 0.11668363958597183,
      "learning_rate": 0.00019949833631942667,
      "loss": 2.2871,
      "step": 50
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.10313931107521057,
      "learning_rate": 0.00019898643460455594,
      "loss": 2.0328,
      "step": 100
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.10400109738111496,
      "learning_rate": 0.00019847453288968518,
      "loss": 1.9725,
      "step": 150
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.10182460397481918,
      "learning_rate": 0.00019796263117481444,
      "loss": 1.9404,
      "step": 200
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.11704091727733612,
      "learning_rate": 0.0001974507294599437,
      "loss": 1.9441,
      "step": 250
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.0974484458565712,
      "learning_rate": 0.00019693882774507294,
      "loss": 1.9314,
      "step": 300
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.11017909646034241,
      "learning_rate": 0.0001964269260302022,
      "loss": 1.9268,
      "step": 350
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.10670784115791321,
      "learning_rate": 0.00019591502431533147,
      "loss": 1.9333,
      "step": 400
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.1074797511100769,
      "learning_rate": 0.0001954031226004607,
      "loss": 1.9405,
      "step": 450
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.09984756261110306,
      "learning_rate": 0.00019489122088558997,
      "loss": 1.9361,
      "step": 500
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.10806844383478165,
      "learning_rate": 0.00019437931917071924,
      "loss": 1.9175,
      "step": 550
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.11450127512216568,
      "learning_rate": 0.0001938674174558485,
      "loss": 1.9269,
      "step": 600
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.1030367761850357,
      "learning_rate": 0.00019335551574097774,
      "loss": 1.9122,
      "step": 650
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.12098410725593567,
      "learning_rate": 0.000192843614026107,
      "loss": 1.9069,
      "step": 700
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.10139675438404083,
      "learning_rate": 0.00019233171231123627,
      "loss": 1.9048,
      "step": 750
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.10389839857816696,
      "learning_rate": 0.0001918198105963655,
      "loss": 1.9262,
      "step": 800
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.1004486232995987,
      "learning_rate": 0.00019130790888149477,
      "loss": 1.9275,
      "step": 850
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.11599448323249817,
      "learning_rate": 0.00019079600716662403,
      "loss": 1.9234,
      "step": 900
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.11016251891851425,
      "learning_rate": 0.00019028410545175327,
      "loss": 1.9212,
      "step": 950
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.11184869706630707,
      "learning_rate": 0.00018977220373688253,
      "loss": 1.9044,
      "step": 1000
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.10310959815979004,
      "learning_rate": 0.0001892603020220118,
      "loss": 1.9174,
      "step": 1050
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.11240080744028091,
      "learning_rate": 0.00018874840030714106,
      "loss": 1.9029,
      "step": 1100
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.10978823155164719,
      "learning_rate": 0.00018823649859227027,
      "loss": 1.9235,
      "step": 1150
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.11147576570510864,
      "learning_rate": 0.00018772459687739954,
      "loss": 1.9116,
      "step": 1200
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.1025531142950058,
      "learning_rate": 0.0001872126951625288,
      "loss": 1.9322,
      "step": 1250
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.10249270498752594,
      "learning_rate": 0.00018670079344765804,
      "loss": 1.9035,
      "step": 1300
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.10681525617837906,
      "learning_rate": 0.0001861888917327873,
      "loss": 1.9154,
      "step": 1350
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.1148238480091095,
      "learning_rate": 0.00018567699001791657,
      "loss": 1.9062,
      "step": 1400
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.10902624577283859,
      "learning_rate": 0.00018516508830304583,
      "loss": 1.9032,
      "step": 1450
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.110878586769104,
      "learning_rate": 0.00018465318658817507,
      "loss": 1.9491,
      "step": 1500
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.10857313871383667,
      "learning_rate": 0.00018414128487330433,
      "loss": 1.881,
      "step": 1550
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.1103544682264328,
      "learning_rate": 0.0001836293831584336,
      "loss": 1.926,
      "step": 1600
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.1073707863688469,
      "learning_rate": 0.00018311748144356283,
      "loss": 1.9006,
      "step": 1650
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.11178271472454071,
      "learning_rate": 0.0001826055797286921,
      "loss": 1.9035,
      "step": 1700
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.11138195544481277,
      "learning_rate": 0.00018209367801382136,
      "loss": 1.9048,
      "step": 1750
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.11676079034805298,
      "learning_rate": 0.0001815817762989506,
      "loss": 1.9043,
      "step": 1800
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.11337202042341232,
      "learning_rate": 0.00018106987458407986,
      "loss": 1.9154,
      "step": 1850
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.10572221875190735,
      "learning_rate": 0.00018055797286920913,
      "loss": 1.8991,
      "step": 1900
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.1084195002913475,
      "learning_rate": 0.0001800460711543384,
      "loss": 1.917,
      "step": 1950
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.11653155833482742,
      "learning_rate": 0.00017953416943946763,
      "loss": 1.9136,
      "step": 2000
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.11754047125577927,
      "learning_rate": 0.0001790222677245969,
      "loss": 1.9118,
      "step": 2050
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.11066429316997528,
      "learning_rate": 0.00017851036600972616,
      "loss": 1.8998,
      "step": 2100
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.11417108029127121,
      "learning_rate": 0.0001779984642948554,
      "loss": 1.93,
      "step": 2150
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.11260686069726944,
      "learning_rate": 0.00017748656257998466,
      "loss": 1.9115,
      "step": 2200
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.12334055453538895,
      "learning_rate": 0.00017697466086511392,
      "loss": 1.9127,
      "step": 2250
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.11044681072235107,
      "learning_rate": 0.00017646275915024316,
      "loss": 1.9146,
      "step": 2300
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.10399115085601807,
      "learning_rate": 0.0001759508574353724,
      "loss": 1.8867,
      "step": 2350
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.11019304394721985,
      "learning_rate": 0.00017543895572050166,
      "loss": 1.8917,
      "step": 2400
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.10316334664821625,
      "learning_rate": 0.00017492705400563093,
      "loss": 1.9111,
      "step": 2450
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.10528971254825592,
      "learning_rate": 0.00017441515229076017,
      "loss": 1.9009,
      "step": 2500
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.11435454338788986,
      "learning_rate": 0.00017390325057588943,
      "loss": 1.9179,
      "step": 2550
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.12059924751520157,
      "learning_rate": 0.0001733913488610187,
      "loss": 1.9102,
      "step": 2600
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.11410094052553177,
      "learning_rate": 0.00017287944714614793,
      "loss": 1.9294,
      "step": 2650
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.1079048216342926,
      "learning_rate": 0.0001723675454312772,
      "loss": 1.9129,
      "step": 2700
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.11443056911230087,
      "learning_rate": 0.00017185564371640646,
      "loss": 1.9133,
      "step": 2750
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.11168409138917923,
      "learning_rate": 0.00017134374200153572,
      "loss": 1.8984,
      "step": 2800
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.11097941547632217,
      "learning_rate": 0.00017083184028666496,
      "loss": 1.9039,
      "step": 2850
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.1201745793223381,
      "learning_rate": 0.00017031993857179423,
      "loss": 1.888,
      "step": 2900
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.11868730932474136,
      "learning_rate": 0.0001698080368569235,
      "loss": 1.9056,
      "step": 2950
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.10832387208938599,
      "learning_rate": 0.00016929613514205273,
      "loss": 1.8844,
      "step": 3000
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.11555834114551544,
      "learning_rate": 0.000168784233427182,
      "loss": 1.8918,
      "step": 3050
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.1281592845916748,
      "learning_rate": 0.00016827233171231126,
      "loss": 1.9295,
      "step": 3100
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.11090805381536484,
      "learning_rate": 0.0001677604299974405,
      "loss": 1.8961,
      "step": 3150
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.11320167034864426,
      "learning_rate": 0.00016724852828256976,
      "loss": 1.9211,
      "step": 3200
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.1167677640914917,
      "learning_rate": 0.00016673662656769902,
      "loss": 1.8948,
      "step": 3250
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.11356101185083389,
      "learning_rate": 0.00016622472485282829,
      "loss": 1.9104,
      "step": 3300
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.1112348660826683,
      "learning_rate": 0.00016571282313795752,
      "loss": 1.9168,
      "step": 3350
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.10998089611530304,
      "learning_rate": 0.0001652009214230868,
      "loss": 1.9042,
      "step": 3400
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.10608096420764923,
      "learning_rate": 0.00016468901970821605,
      "loss": 1.91,
      "step": 3450
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.116080641746521,
      "learning_rate": 0.00016417711799334526,
      "loss": 1.9096,
      "step": 3500
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.10792884975671768,
      "learning_rate": 0.00016366521627847453,
      "loss": 1.8935,
      "step": 3550
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.11587298661470413,
      "learning_rate": 0.0001631533145636038,
      "loss": 1.9179,
      "step": 3600
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.11422478407621384,
      "learning_rate": 0.00016264141284873305,
      "loss": 1.8891,
      "step": 3650
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.10883477330207825,
      "learning_rate": 0.0001621295111338623,
      "loss": 1.8853,
      "step": 3700
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.11525045335292816,
      "learning_rate": 0.00016161760941899156,
      "loss": 1.9325,
      "step": 3750
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.1322801262140274,
      "learning_rate": 0.00016110570770412082,
      "loss": 1.8917,
      "step": 3800
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.11288075149059296,
      "learning_rate": 0.00016059380598925006,
      "loss": 1.9217,
      "step": 3850
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.11232601851224899,
      "learning_rate": 0.00016008190427437932,
      "loss": 1.8951,
      "step": 3900
    },
    {
      "epoch": 1.011008,
      "grad_norm": 0.14015670120716095,
      "learning_rate": 0.00015957000255950859,
      "loss": 1.8747,
      "step": 3950
    },
    {
      "epoch": 1.023808,
      "grad_norm": 0.11556243896484375,
      "learning_rate": 0.00015905810084463782,
      "loss": 1.8912,
      "step": 4000
    },
    {
      "epoch": 1.036608,
      "grad_norm": 0.1333107054233551,
      "learning_rate": 0.0001585461991297671,
      "loss": 1.9075,
      "step": 4050
    },
    {
      "epoch": 1.049408,
      "grad_norm": 0.10895098000764847,
      "learning_rate": 0.00015803429741489635,
      "loss": 1.8803,
      "step": 4100
    },
    {
      "epoch": 1.062208,
      "grad_norm": 0.12764988839626312,
      "learning_rate": 0.00015752239570002562,
      "loss": 1.8964,
      "step": 4150
    },
    {
      "epoch": 1.075008,
      "grad_norm": 0.12846654653549194,
      "learning_rate": 0.00015701049398515485,
      "loss": 1.9063,
      "step": 4200
    },
    {
      "epoch": 1.0878079999999999,
      "grad_norm": 0.1256396770477295,
      "learning_rate": 0.00015649859227028412,
      "loss": 1.9082,
      "step": 4250
    },
    {
      "epoch": 1.100608,
      "grad_norm": 0.12358731031417847,
      "learning_rate": 0.00015598669055541338,
      "loss": 1.9278,
      "step": 4300
    },
    {
      "epoch": 1.113408,
      "grad_norm": 0.12016578763723373,
      "learning_rate": 0.00015547478884054262,
      "loss": 1.8883,
      "step": 4350
    },
    {
      "epoch": 1.126208,
      "grad_norm": 0.12399078905582428,
      "learning_rate": 0.00015496288712567188,
      "loss": 1.9094,
      "step": 4400
    },
    {
      "epoch": 1.139008,
      "grad_norm": 0.11245372891426086,
      "learning_rate": 0.00015445098541080115,
      "loss": 1.8853,
      "step": 4450
    },
    {
      "epoch": 1.151808,
      "grad_norm": 0.11073344200849533,
      "learning_rate": 0.00015393908369593039,
      "loss": 1.9073,
      "step": 4500
    },
    {
      "epoch": 1.164608,
      "grad_norm": 0.11386581510305405,
      "learning_rate": 0.00015342718198105965,
      "loss": 1.8997,
      "step": 4550
    },
    {
      "epoch": 1.177408,
      "grad_norm": 0.11183847486972809,
      "learning_rate": 0.00015291528026618891,
      "loss": 1.8756,
      "step": 4600
    },
    {
      "epoch": 1.190208,
      "grad_norm": 0.11930420249700546,
      "learning_rate": 0.00015240337855131815,
      "loss": 1.8768,
      "step": 4650
    },
    {
      "epoch": 1.203008,
      "grad_norm": 0.11404210329055786,
      "learning_rate": 0.0001518914768364474,
      "loss": 1.9177,
      "step": 4700
    },
    {
      "epoch": 1.215808,
      "grad_norm": 0.11229496449232101,
      "learning_rate": 0.00015137957512157665,
      "loss": 1.8832,
      "step": 4750
    },
    {
      "epoch": 1.228608,
      "grad_norm": 0.11742336302995682,
      "learning_rate": 0.00015086767340670592,
      "loss": 1.8999,
      "step": 4800
    },
    {
      "epoch": 1.241408,
      "grad_norm": 0.11607132107019424,
      "learning_rate": 0.00015035577169183515,
      "loss": 1.9151,
      "step": 4850
    },
    {
      "epoch": 1.254208,
      "grad_norm": 0.12481022626161575,
      "learning_rate": 0.00014984386997696442,
      "loss": 1.8956,
      "step": 4900
    },
    {
      "epoch": 1.2670080000000001,
      "grad_norm": 0.11088244616985321,
      "learning_rate": 0.00014933196826209368,
      "loss": 1.9062,
      "step": 4950
    },
    {
      "epoch": 1.279808,
      "grad_norm": 0.13006553053855896,
      "learning_rate": 0.00014882006654722295,
      "loss": 1.8972,
      "step": 5000
    },
    {
      "epoch": 1.292608,
      "grad_norm": 0.12216916680335999,
      "learning_rate": 0.00014830816483235218,
      "loss": 1.9054,
      "step": 5050
    },
    {
      "epoch": 1.305408,
      "grad_norm": 0.10491223633289337,
      "learning_rate": 0.00014779626311748145,
      "loss": 1.893,
      "step": 5100
    },
    {
      "epoch": 1.318208,
      "grad_norm": 0.1276608556509018,
      "learning_rate": 0.0001472843614026107,
      "loss": 1.8879,
      "step": 5150
    },
    {
      "epoch": 1.331008,
      "grad_norm": 0.10326120257377625,
      "learning_rate": 0.00014677245968773995,
      "loss": 1.8981,
      "step": 5200
    },
    {
      "epoch": 1.3438080000000001,
      "grad_norm": 0.1168118342757225,
      "learning_rate": 0.00014626055797286921,
      "loss": 1.9148,
      "step": 5250
    },
    {
      "epoch": 1.356608,
      "grad_norm": 0.11881959438323975,
      "learning_rate": 0.00014574865625799848,
      "loss": 1.8998,
      "step": 5300
    },
    {
      "epoch": 1.369408,
      "grad_norm": 0.12214071303606033,
      "learning_rate": 0.00014523675454312772,
      "loss": 1.9083,
      "step": 5350
    },
    {
      "epoch": 1.3822079999999999,
      "grad_norm": 0.1188107505440712,
      "learning_rate": 0.00014472485282825698,
      "loss": 1.8967,
      "step": 5400
    },
    {
      "epoch": 1.395008,
      "grad_norm": 0.1194409653544426,
      "learning_rate": 0.00014421295111338625,
      "loss": 1.9007,
      "step": 5450
    },
    {
      "epoch": 1.407808,
      "grad_norm": 0.11441067606210709,
      "learning_rate": 0.0001437010493985155,
      "loss": 1.8949,
      "step": 5500
    },
    {
      "epoch": 1.420608,
      "grad_norm": 0.11543749272823334,
      "learning_rate": 0.00014318914768364475,
      "loss": 1.8805,
      "step": 5550
    },
    {
      "epoch": 1.433408,
      "grad_norm": 0.11912860721349716,
      "learning_rate": 0.000142677245968774,
      "loss": 1.9042,
      "step": 5600
    },
    {
      "epoch": 1.446208,
      "grad_norm": 0.10781756043434143,
      "learning_rate": 0.00014216534425390328,
      "loss": 1.8739,
      "step": 5650
    },
    {
      "epoch": 1.459008,
      "grad_norm": 0.12706266343593597,
      "learning_rate": 0.0001416534425390325,
      "loss": 1.8772,
      "step": 5700
    },
    {
      "epoch": 1.471808,
      "grad_norm": 0.11138106882572174,
      "learning_rate": 0.00014114154082416178,
      "loss": 1.8984,
      "step": 5750
    },
    {
      "epoch": 1.484608,
      "grad_norm": 0.12088313698768616,
      "learning_rate": 0.00014062963910929104,
      "loss": 1.9041,
      "step": 5800
    },
    {
      "epoch": 1.497408,
      "grad_norm": 0.11066367477178574,
      "learning_rate": 0.00014011773739442028,
      "loss": 1.8934,
      "step": 5850
    },
    {
      "epoch": 1.510208,
      "grad_norm": 0.11186940968036652,
      "learning_rate": 0.00013960583567954952,
      "loss": 1.9085,
      "step": 5900
    },
    {
      "epoch": 1.523008,
      "grad_norm": 0.11541182547807693,
      "learning_rate": 0.00013909393396467878,
      "loss": 1.8998,
      "step": 5950
    },
    {
      "epoch": 1.5358079999999998,
      "grad_norm": 0.12066196650266647,
      "learning_rate": 0.00013858203224980804,
      "loss": 1.8869,
      "step": 6000
    },
    {
      "epoch": 1.548608,
      "grad_norm": 0.11123862117528915,
      "learning_rate": 0.00013807013053493728,
      "loss": 1.9219,
      "step": 6050
    },
    {
      "epoch": 1.5614080000000001,
      "grad_norm": 0.12517398595809937,
      "learning_rate": 0.00013755822882006655,
      "loss": 1.8979,
      "step": 6100
    },
    {
      "epoch": 1.574208,
      "grad_norm": 0.11766988784074783,
      "learning_rate": 0.0001370463271051958,
      "loss": 1.8952,
      "step": 6150
    },
    {
      "epoch": 1.587008,
      "grad_norm": 0.12442253530025482,
      "learning_rate": 0.00013653442539032505,
      "loss": 1.8995,
      "step": 6200
    },
    {
      "epoch": 1.599808,
      "grad_norm": 0.13370947539806366,
      "learning_rate": 0.0001360225236754543,
      "loss": 1.9004,
      "step": 6250
    },
    {
      "epoch": 1.612608,
      "grad_norm": 0.12026455253362656,
      "learning_rate": 0.00013551062196058358,
      "loss": 1.9002,
      "step": 6300
    },
    {
      "epoch": 1.625408,
      "grad_norm": 0.11250168830156326,
      "learning_rate": 0.00013499872024571284,
      "loss": 1.9027,
      "step": 6350
    },
    {
      "epoch": 1.638208,
      "grad_norm": 0.11239110678434372,
      "learning_rate": 0.00013448681853084208,
      "loss": 1.9005,
      "step": 6400
    },
    {
      "epoch": 1.651008,
      "grad_norm": 0.11766649037599564,
      "learning_rate": 0.00013397491681597134,
      "loss": 1.911,
      "step": 6450
    },
    {
      "epoch": 1.663808,
      "grad_norm": 0.11119525134563446,
      "learning_rate": 0.0001334630151011006,
      "loss": 1.9016,
      "step": 6500
    },
    {
      "epoch": 1.6766079999999999,
      "grad_norm": 0.11904126405715942,
      "learning_rate": 0.00013295111338622984,
      "loss": 1.9191,
      "step": 6550
    },
    {
      "epoch": 1.689408,
      "grad_norm": 0.11956054717302322,
      "learning_rate": 0.0001324392116713591,
      "loss": 1.907,
      "step": 6600
    },
    {
      "epoch": 1.7022080000000002,
      "grad_norm": 0.12270307540893555,
      "learning_rate": 0.00013192730995648837,
      "loss": 1.9092,
      "step": 6650
    },
    {
      "epoch": 1.715008,
      "grad_norm": 0.12015901505947113,
      "learning_rate": 0.0001314154082416176,
      "loss": 1.898,
      "step": 6700
    },
    {
      "epoch": 1.727808,
      "grad_norm": 0.11825137585401535,
      "learning_rate": 0.00013090350652674687,
      "loss": 1.8912,
      "step": 6750
    },
    {
      "epoch": 1.740608,
      "grad_norm": 0.12391684204339981,
      "learning_rate": 0.00013039160481187614,
      "loss": 1.8695,
      "step": 6800
    },
    {
      "epoch": 1.7534079999999999,
      "grad_norm": 0.11580060422420502,
      "learning_rate": 0.0001298797030970054,
      "loss": 1.8894,
      "step": 6850
    },
    {
      "epoch": 1.766208,
      "grad_norm": 0.13511286675930023,
      "learning_rate": 0.00012936780138213464,
      "loss": 1.8612,
      "step": 6900
    },
    {
      "epoch": 1.7790080000000001,
      "grad_norm": 0.12770287692546844,
      "learning_rate": 0.0001288558996672639,
      "loss": 1.9002,
      "step": 6950
    },
    {
      "epoch": 1.791808,
      "grad_norm": 0.12283562868833542,
      "learning_rate": 0.00012834399795239314,
      "loss": 1.9082,
      "step": 7000
    },
    {
      "epoch": 1.804608,
      "grad_norm": 0.12872110307216644,
      "learning_rate": 0.0001278320962375224,
      "loss": 1.8828,
      "step": 7050
    },
    {
      "epoch": 1.817408,
      "grad_norm": 0.11395183205604553,
      "learning_rate": 0.00012732019452265164,
      "loss": 1.8953,
      "step": 7100
    },
    {
      "epoch": 1.8302079999999998,
      "grad_norm": 0.1316787302494049,
      "learning_rate": 0.0001268082928077809,
      "loss": 1.8973,
      "step": 7150
    },
    {
      "epoch": 1.843008,
      "grad_norm": 0.11781288683414459,
      "learning_rate": 0.00012629639109291017,
      "loss": 1.8914,
      "step": 7200
    },
    {
      "epoch": 1.8558080000000001,
      "grad_norm": 0.11590505391359329,
      "learning_rate": 0.0001257844893780394,
      "loss": 1.9282,
      "step": 7250
    },
    {
      "epoch": 1.868608,
      "grad_norm": 0.12654300034046173,
      "learning_rate": 0.00012527258766316867,
      "loss": 1.9089,
      "step": 7300
    },
    {
      "epoch": 1.881408,
      "grad_norm": 0.12293968349695206,
      "learning_rate": 0.00012476068594829794,
      "loss": 1.8864,
      "step": 7350
    },
    {
      "epoch": 1.894208,
      "grad_norm": 0.11487430334091187,
      "learning_rate": 0.00012424878423342717,
      "loss": 1.8993,
      "step": 7400
    },
    {
      "epoch": 1.907008,
      "grad_norm": 0.117039754986763,
      "learning_rate": 0.00012373688251855644,
      "loss": 1.8871,
      "step": 7450
    },
    {
      "epoch": 1.919808,
      "grad_norm": 0.1208852082490921,
      "learning_rate": 0.0001232249808036857,
      "loss": 1.9166,
      "step": 7500
    },
    {
      "epoch": 1.932608,
      "grad_norm": 0.11904232203960419,
      "learning_rate": 0.00012271307908881494,
      "loss": 1.8986,
      "step": 7550
    },
    {
      "epoch": 1.945408,
      "grad_norm": 0.12117605656385422,
      "learning_rate": 0.0001222011773739442,
      "loss": 1.9058,
      "step": 7600
    },
    {
      "epoch": 1.958208,
      "grad_norm": 0.11263155937194824,
      "learning_rate": 0.00012168927565907347,
      "loss": 1.9137,
      "step": 7650
    },
    {
      "epoch": 1.9710079999999999,
      "grad_norm": 0.12433572113513947,
      "learning_rate": 0.00012117737394420272,
      "loss": 1.8766,
      "step": 7700
    },
    {
      "epoch": 1.983808,
      "grad_norm": 0.11577647924423218,
      "learning_rate": 0.00012066547222933198,
      "loss": 1.9044,
      "step": 7750
    },
    {
      "epoch": 1.9966080000000002,
      "grad_norm": 0.13219033181667328,
      "learning_rate": 0.00012015357051446123,
      "loss": 1.8898,
      "step": 7800
    },
    {
      "epoch": 2.009216,
      "grad_norm": 0.1164860725402832,
      "learning_rate": 0.00011964166879959049,
      "loss": 1.8856,
      "step": 7850
    },
    {
      "epoch": 2.022016,
      "grad_norm": 0.11509676277637482,
      "learning_rate": 0.00011912976708471975,
      "loss": 1.9123,
      "step": 7900
    },
    {
      "epoch": 2.034816,
      "grad_norm": 0.13039323687553406,
      "learning_rate": 0.000118617865369849,
      "loss": 1.8733,
      "step": 7950
    },
    {
      "epoch": 2.047616,
      "grad_norm": 0.12554068863391876,
      "learning_rate": 0.00011810596365497825,
      "loss": 1.8906,
      "step": 8000
    },
    {
      "epoch": 2.060416,
      "grad_norm": 0.14006365835666656,
      "learning_rate": 0.00011759406194010752,
      "loss": 1.8942,
      "step": 8050
    },
    {
      "epoch": 2.073216,
      "grad_norm": 0.1118573322892189,
      "learning_rate": 0.00011708216022523677,
      "loss": 1.8824,
      "step": 8100
    },
    {
      "epoch": 2.086016,
      "grad_norm": 0.123493492603302,
      "learning_rate": 0.00011657025851036603,
      "loss": 1.8948,
      "step": 8150
    },
    {
      "epoch": 2.098816,
      "grad_norm": 0.12079509347677231,
      "learning_rate": 0.00011605835679549525,
      "loss": 1.946,
      "step": 8200
    },
    {
      "epoch": 2.111616,
      "grad_norm": 0.1347978711128235,
      "learning_rate": 0.00011554645508062452,
      "loss": 1.876,
      "step": 8250
    },
    {
      "epoch": 2.124416,
      "grad_norm": 0.1266825646162033,
      "learning_rate": 0.00011503455336575377,
      "loss": 1.866,
      "step": 8300
    },
    {
      "epoch": 2.137216,
      "grad_norm": 0.13928893208503723,
      "learning_rate": 0.00011452265165088303,
      "loss": 1.8904,
      "step": 8350
    },
    {
      "epoch": 2.150016,
      "grad_norm": 0.12351720780134201,
      "learning_rate": 0.00011401074993601228,
      "loss": 1.8949,
      "step": 8400
    },
    {
      "epoch": 2.162816,
      "grad_norm": 0.1263718605041504,
      "learning_rate": 0.00011349884822114154,
      "loss": 1.8954,
      "step": 8450
    },
    {
      "epoch": 2.1756159999999998,
      "grad_norm": 0.12144503742456436,
      "learning_rate": 0.0001129869465062708,
      "loss": 1.8974,
      "step": 8500
    },
    {
      "epoch": 2.188416,
      "grad_norm": 0.13022467494010925,
      "learning_rate": 0.00011247504479140005,
      "loss": 1.8788,
      "step": 8550
    },
    {
      "epoch": 2.201216,
      "grad_norm": 0.11229725182056427,
      "learning_rate": 0.00011196314307652931,
      "loss": 1.8898,
      "step": 8600
    },
    {
      "epoch": 2.214016,
      "grad_norm": 0.11566787213087082,
      "learning_rate": 0.00011145124136165857,
      "loss": 1.8864,
      "step": 8650
    },
    {
      "epoch": 2.226816,
      "grad_norm": 0.1162690594792366,
      "learning_rate": 0.00011093933964678782,
      "loss": 1.9099,
      "step": 8700
    },
    {
      "epoch": 2.239616,
      "grad_norm": 0.12491164356470108,
      "learning_rate": 0.00011042743793191708,
      "loss": 1.8957,
      "step": 8750
    },
    {
      "epoch": 2.252416,
      "grad_norm": 0.12357300519943237,
      "learning_rate": 0.00010991553621704633,
      "loss": 1.8975,
      "step": 8800
    },
    {
      "epoch": 2.265216,
      "grad_norm": 0.1246638223528862,
      "learning_rate": 0.0001094036345021756,
      "loss": 1.8912,
      "step": 8850
    },
    {
      "epoch": 2.278016,
      "grad_norm": 0.12768837809562683,
      "learning_rate": 0.00010889173278730485,
      "loss": 1.8964,
      "step": 8900
    },
    {
      "epoch": 2.290816,
      "grad_norm": 0.12076853960752487,
      "learning_rate": 0.0001083798310724341,
      "loss": 1.8891,
      "step": 8950
    },
    {
      "epoch": 2.303616,
      "grad_norm": 0.13863974809646606,
      "learning_rate": 0.00010786792935756336,
      "loss": 1.8913,
      "step": 9000
    },
    {
      "epoch": 2.316416,
      "grad_norm": 0.12426359206438065,
      "learning_rate": 0.00010735602764269261,
      "loss": 1.9006,
      "step": 9050
    },
    {
      "epoch": 2.329216,
      "grad_norm": 0.1218574121594429,
      "learning_rate": 0.00010684412592782186,
      "loss": 1.8835,
      "step": 9100
    },
    {
      "epoch": 2.342016,
      "grad_norm": 0.11498428881168365,
      "learning_rate": 0.00010633222421295113,
      "loss": 1.8825,
      "step": 9150
    },
    {
      "epoch": 2.354816,
      "grad_norm": 0.1357763260602951,
      "learning_rate": 0.00010582032249808038,
      "loss": 1.8617,
      "step": 9200
    },
    {
      "epoch": 2.367616,
      "grad_norm": 0.12342766672372818,
      "learning_rate": 0.00010530842078320964,
      "loss": 1.8884,
      "step": 9250
    },
    {
      "epoch": 2.380416,
      "grad_norm": 0.11696965247392654,
      "learning_rate": 0.0001047965190683389,
      "loss": 1.8714,
      "step": 9300
    },
    {
      "epoch": 2.393216,
      "grad_norm": 0.15136069059371948,
      "learning_rate": 0.00010428461735346813,
      "loss": 1.888,
      "step": 9350
    },
    {
      "epoch": 2.406016,
      "grad_norm": 0.1282702386379242,
      "learning_rate": 0.00010377271563859738,
      "loss": 1.8623,
      "step": 9400
    },
    {
      "epoch": 2.418816,
      "grad_norm": 0.12210643291473389,
      "learning_rate": 0.00010326081392372665,
      "loss": 1.8861,
      "step": 9450
    },
    {
      "epoch": 2.431616,
      "grad_norm": 0.12302736937999725,
      "learning_rate": 0.0001027489122088559,
      "loss": 1.8786,
      "step": 9500
    },
    {
      "epoch": 2.444416,
      "grad_norm": 0.11259391903877258,
      "learning_rate": 0.00010223701049398515,
      "loss": 1.8755,
      "step": 9550
    },
    {
      "epoch": 2.457216,
      "grad_norm": 0.11894263327121735,
      "learning_rate": 0.00010172510877911441,
      "loss": 1.8852,
      "step": 9600
    },
    {
      "epoch": 2.470016,
      "grad_norm": 0.12770690023899078,
      "learning_rate": 0.00010121320706424366,
      "loss": 1.9084,
      "step": 9650
    },
    {
      "epoch": 2.482816,
      "grad_norm": 0.12955661118030548,
      "learning_rate": 0.00010070130534937293,
      "loss": 1.8688,
      "step": 9700
    },
    {
      "epoch": 2.495616,
      "grad_norm": 0.14330220222473145,
      "learning_rate": 0.00010018940363450218,
      "loss": 1.8965,
      "step": 9750
    },
    {
      "epoch": 2.508416,
      "grad_norm": 0.12780456244945526,
      "learning_rate": 9.967750191963143e-05,
      "loss": 1.8849,
      "step": 9800
    },
    {
      "epoch": 2.521216,
      "grad_norm": 0.13013270497322083,
      "learning_rate": 9.916560020476069e-05,
      "loss": 1.8936,
      "step": 9850
    },
    {
      "epoch": 2.5340160000000003,
      "grad_norm": 0.1290498971939087,
      "learning_rate": 9.865369848988994e-05,
      "loss": 1.8802,
      "step": 9900
    },
    {
      "epoch": 2.5468159999999997,
      "grad_norm": 0.1302109807729721,
      "learning_rate": 9.814179677501921e-05,
      "loss": 1.8729,
      "step": 9950
    },
    {
      "epoch": 2.559616,
      "grad_norm": 0.12712572515010834,
      "learning_rate": 9.762989506014846e-05,
      "loss": 1.8907,
      "step": 10000
    },
    {
      "epoch": 2.572416,
      "grad_norm": 0.1318124383687973,
      "learning_rate": 9.711799334527771e-05,
      "loss": 1.892,
      "step": 10050
    },
    {
      "epoch": 2.585216,
      "grad_norm": 0.1380062997341156,
      "learning_rate": 9.660609163040697e-05,
      "loss": 1.8911,
      "step": 10100
    },
    {
      "epoch": 2.598016,
      "grad_norm": 0.11283000558614731,
      "learning_rate": 9.609418991553622e-05,
      "loss": 1.9022,
      "step": 10150
    },
    {
      "epoch": 2.610816,
      "grad_norm": 0.12122774124145508,
      "learning_rate": 9.558228820066549e-05,
      "loss": 1.8909,
      "step": 10200
    },
    {
      "epoch": 2.623616,
      "grad_norm": 0.12520194053649902,
      "learning_rate": 9.507038648579473e-05,
      "loss": 1.8781,
      "step": 10250
    },
    {
      "epoch": 2.636416,
      "grad_norm": 0.12739816308021545,
      "learning_rate": 9.455848477092398e-05,
      "loss": 1.9045,
      "step": 10300
    },
    {
      "epoch": 2.649216,
      "grad_norm": 0.1419898420572281,
      "learning_rate": 9.404658305605324e-05,
      "loss": 1.8951,
      "step": 10350
    },
    {
      "epoch": 2.662016,
      "grad_norm": 0.1335960179567337,
      "learning_rate": 9.353468134118249e-05,
      "loss": 1.9079,
      "step": 10400
    },
    {
      "epoch": 2.674816,
      "grad_norm": 0.1331537961959839,
      "learning_rate": 9.302277962631176e-05,
      "loss": 1.9146,
      "step": 10450
    },
    {
      "epoch": 2.6876160000000002,
      "grad_norm": 0.12534672021865845,
      "learning_rate": 9.2510877911441e-05,
      "loss": 1.8948,
      "step": 10500
    },
    {
      "epoch": 2.700416,
      "grad_norm": 0.12714511156082153,
      "learning_rate": 9.199897619657026e-05,
      "loss": 1.9293,
      "step": 10550
    },
    {
      "epoch": 2.713216,
      "grad_norm": 0.12127542495727539,
      "learning_rate": 9.148707448169952e-05,
      "loss": 1.9,
      "step": 10600
    },
    {
      "epoch": 2.726016,
      "grad_norm": 0.12520572543144226,
      "learning_rate": 9.097517276682877e-05,
      "loss": 1.867,
      "step": 10650
    },
    {
      "epoch": 2.738816,
      "grad_norm": 0.1346098631620407,
      "learning_rate": 9.046327105195804e-05,
      "loss": 1.8947,
      "step": 10700
    },
    {
      "epoch": 2.751616,
      "grad_norm": 0.12245121598243713,
      "learning_rate": 8.995136933708729e-05,
      "loss": 1.8841,
      "step": 10750
    },
    {
      "epoch": 2.7644159999999998,
      "grad_norm": 0.13593566417694092,
      "learning_rate": 8.943946762221654e-05,
      "loss": 1.8975,
      "step": 10800
    },
    {
      "epoch": 2.777216,
      "grad_norm": 0.11941295117139816,
      "learning_rate": 8.892756590734579e-05,
      "loss": 1.8979,
      "step": 10850
    },
    {
      "epoch": 2.790016,
      "grad_norm": 0.1260760873556137,
      "learning_rate": 8.841566419247504e-05,
      "loss": 1.8759,
      "step": 10900
    },
    {
      "epoch": 2.802816,
      "grad_norm": 0.11892002820968628,
      "learning_rate": 8.79037624776043e-05,
      "loss": 1.8913,
      "step": 10950
    },
    {
      "epoch": 2.815616,
      "grad_norm": 0.12405868619680405,
      "learning_rate": 8.739186076273355e-05,
      "loss": 1.901,
      "step": 11000
    },
    {
      "epoch": 2.828416,
      "grad_norm": 0.12727531790733337,
      "learning_rate": 8.687995904786282e-05,
      "loss": 1.9037,
      "step": 11050
    },
    {
      "epoch": 2.841216,
      "grad_norm": 0.1287088841199875,
      "learning_rate": 8.636805733299207e-05,
      "loss": 1.8869,
      "step": 11100
    },
    {
      "epoch": 2.854016,
      "grad_norm": 0.1333063244819641,
      "learning_rate": 8.585615561812132e-05,
      "loss": 1.915,
      "step": 11150
    },
    {
      "epoch": 2.866816,
      "grad_norm": 0.12404534220695496,
      "learning_rate": 8.534425390325059e-05,
      "loss": 1.8866,
      "step": 11200
    },
    {
      "epoch": 2.879616,
      "grad_norm": 0.13482558727264404,
      "learning_rate": 8.483235218837984e-05,
      "loss": 1.8663,
      "step": 11250
    },
    {
      "epoch": 2.892416,
      "grad_norm": 0.11955301463603973,
      "learning_rate": 8.43204504735091e-05,
      "loss": 1.8854,
      "step": 11300
    },
    {
      "epoch": 2.9052160000000002,
      "grad_norm": 0.12253030389547348,
      "learning_rate": 8.380854875863835e-05,
      "loss": 1.867,
      "step": 11350
    },
    {
      "epoch": 2.918016,
      "grad_norm": 0.14461447298526764,
      "learning_rate": 8.32966470437676e-05,
      "loss": 1.8923,
      "step": 11400
    },
    {
      "epoch": 2.930816,
      "grad_norm": 0.12161935865879059,
      "learning_rate": 8.278474532889685e-05,
      "loss": 1.8997,
      "step": 11450
    },
    {
      "epoch": 2.943616,
      "grad_norm": 0.13719624280929565,
      "learning_rate": 8.22728436140261e-05,
      "loss": 1.9187,
      "step": 11500
    },
    {
      "epoch": 2.956416,
      "grad_norm": 0.12719768285751343,
      "learning_rate": 8.176094189915537e-05,
      "loss": 1.9021,
      "step": 11550
    },
    {
      "epoch": 2.969216,
      "grad_norm": 0.1169866994023323,
      "learning_rate": 8.124904018428462e-05,
      "loss": 1.9157,
      "step": 11600
    },
    {
      "epoch": 2.982016,
      "grad_norm": 0.12147817760705948,
      "learning_rate": 8.073713846941387e-05,
      "loss": 1.8738,
      "step": 11650
    },
    {
      "epoch": 2.994816,
      "grad_norm": 0.1211203932762146,
      "learning_rate": 8.022523675454313e-05,
      "loss": 1.8803,
      "step": 11700
    },
    {
      "epoch": 3.007424,
      "grad_norm": 0.12763801217079163,
      "learning_rate": 7.971333503967238e-05,
      "loss": 1.9064,
      "step": 11750
    },
    {
      "epoch": 3.020224,
      "grad_norm": 0.12213727831840515,
      "learning_rate": 7.920143332480165e-05,
      "loss": 1.8972,
      "step": 11800
    },
    {
      "epoch": 3.033024,
      "grad_norm": 0.12058593332767487,
      "learning_rate": 7.86895316099309e-05,
      "loss": 1.8897,
      "step": 11850
    },
    {
      "epoch": 3.045824,
      "grad_norm": 0.12534716725349426,
      "learning_rate": 7.817762989506015e-05,
      "loss": 1.8814,
      "step": 11900
    },
    {
      "epoch": 3.058624,
      "grad_norm": 0.12299204617738724,
      "learning_rate": 7.766572818018941e-05,
      "loss": 1.8938,
      "step": 11950
    },
    {
      "epoch": 3.071424,
      "grad_norm": 0.11938367038965225,
      "learning_rate": 7.715382646531865e-05,
      "loss": 1.8855,
      "step": 12000
    },
    {
      "epoch": 3.084224,
      "grad_norm": 0.14339296519756317,
      "learning_rate": 7.664192475044792e-05,
      "loss": 1.9065,
      "step": 12050
    },
    {
      "epoch": 3.097024,
      "grad_norm": 0.13912178575992584,
      "learning_rate": 7.613002303557717e-05,
      "loss": 1.8933,
      "step": 12100
    },
    {
      "epoch": 3.109824,
      "grad_norm": 0.12371397763490677,
      "learning_rate": 7.561812132070643e-05,
      "loss": 1.8945,
      "step": 12150
    },
    {
      "epoch": 3.122624,
      "grad_norm": 0.12468579411506653,
      "learning_rate": 7.510621960583568e-05,
      "loss": 1.9035,
      "step": 12200
    },
    {
      "epoch": 3.135424,
      "grad_norm": 0.1279815137386322,
      "learning_rate": 7.459431789096493e-05,
      "loss": 1.8854,
      "step": 12250
    },
    {
      "epoch": 3.148224,
      "grad_norm": 0.13219709694385529,
      "learning_rate": 7.40824161760942e-05,
      "loss": 1.8922,
      "step": 12300
    },
    {
      "epoch": 3.161024,
      "grad_norm": 0.12817253172397614,
      "learning_rate": 7.357051446122345e-05,
      "loss": 1.9033,
      "step": 12350
    },
    {
      "epoch": 3.173824,
      "grad_norm": 0.13173726201057434,
      "learning_rate": 7.305861274635271e-05,
      "loss": 1.891,
      "step": 12400
    },
    {
      "epoch": 3.186624,
      "grad_norm": 0.1330268830060959,
      "learning_rate": 7.254671103148196e-05,
      "loss": 1.9024,
      "step": 12450
    },
    {
      "epoch": 3.199424,
      "grad_norm": 0.1243254542350769,
      "learning_rate": 7.203480931661121e-05,
      "loss": 1.8783,
      "step": 12500
    },
    {
      "epoch": 3.212224,
      "grad_norm": 0.13140489161014557,
      "learning_rate": 7.152290760174048e-05,
      "loss": 1.8842,
      "step": 12550
    },
    {
      "epoch": 3.225024,
      "grad_norm": 0.13231533765792847,
      "learning_rate": 7.101100588686972e-05,
      "loss": 1.8907,
      "step": 12600
    },
    {
      "epoch": 3.237824,
      "grad_norm": 0.13799038529396057,
      "learning_rate": 7.049910417199898e-05,
      "loss": 1.8802,
      "step": 12650
    },
    {
      "epoch": 3.250624,
      "grad_norm": 0.1187974363565445,
      "learning_rate": 6.998720245712823e-05,
      "loss": 1.8647,
      "step": 12700
    },
    {
      "epoch": 3.263424,
      "grad_norm": 0.12443900853395462,
      "learning_rate": 6.947530074225748e-05,
      "loss": 1.8953,
      "step": 12750
    },
    {
      "epoch": 3.276224,
      "grad_norm": 0.13708770275115967,
      "learning_rate": 6.896339902738675e-05,
      "loss": 1.8871,
      "step": 12800
    },
    {
      "epoch": 3.289024,
      "grad_norm": 0.13221342861652374,
      "learning_rate": 6.8451497312516e-05,
      "loss": 1.9128,
      "step": 12850
    },
    {
      "epoch": 3.301824,
      "grad_norm": 0.12909598648548126,
      "learning_rate": 6.793959559764526e-05,
      "loss": 1.8945,
      "step": 12900
    },
    {
      "epoch": 3.3146240000000002,
      "grad_norm": 0.12303005158901215,
      "learning_rate": 6.742769388277451e-05,
      "loss": 1.8597,
      "step": 12950
    },
    {
      "epoch": 3.327424,
      "grad_norm": 0.1194453164935112,
      "learning_rate": 6.691579216790376e-05,
      "loss": 1.8584,
      "step": 13000
    },
    {
      "epoch": 3.340224,
      "grad_norm": 0.1339869201183319,
      "learning_rate": 6.640389045303303e-05,
      "loss": 1.9037,
      "step": 13050
    },
    {
      "epoch": 3.353024,
      "grad_norm": 0.13229715824127197,
      "learning_rate": 6.589198873816228e-05,
      "loss": 1.8855,
      "step": 13100
    },
    {
      "epoch": 3.365824,
      "grad_norm": 0.13813866674900055,
      "learning_rate": 6.538008702329154e-05,
      "loss": 1.9029,
      "step": 13150
    },
    {
      "epoch": 3.378624,
      "grad_norm": 0.14048589766025543,
      "learning_rate": 6.486818530842078e-05,
      "loss": 1.8734,
      "step": 13200
    },
    {
      "epoch": 3.3914239999999998,
      "grad_norm": 0.13503286242485046,
      "learning_rate": 6.435628359355004e-05,
      "loss": 1.889,
      "step": 13250
    },
    {
      "epoch": 3.404224,
      "grad_norm": 0.14410774409770966,
      "learning_rate": 6.38443818786793e-05,
      "loss": 1.9096,
      "step": 13300
    },
    {
      "epoch": 3.417024,
      "grad_norm": 0.12257374078035355,
      "learning_rate": 6.333248016380854e-05,
      "loss": 1.8766,
      "step": 13350
    },
    {
      "epoch": 3.429824,
      "grad_norm": 0.13919973373413086,
      "learning_rate": 6.282057844893781e-05,
      "loss": 1.8857,
      "step": 13400
    },
    {
      "epoch": 3.442624,
      "grad_norm": 0.13167431950569153,
      "learning_rate": 6.230867673406706e-05,
      "loss": 1.886,
      "step": 13450
    },
    {
      "epoch": 3.455424,
      "grad_norm": 0.12769056856632233,
      "learning_rate": 6.179677501919632e-05,
      "loss": 1.8746,
      "step": 13500
    },
    {
      "epoch": 3.468224,
      "grad_norm": 0.12898115813732147,
      "learning_rate": 6.128487330432557e-05,
      "loss": 1.8825,
      "step": 13550
    },
    {
      "epoch": 3.481024,
      "grad_norm": 0.12355770915746689,
      "learning_rate": 6.077297158945483e-05,
      "loss": 1.8692,
      "step": 13600
    },
    {
      "epoch": 3.493824,
      "grad_norm": 0.14007176458835602,
      "learning_rate": 6.026106987458408e-05,
      "loss": 1.8878,
      "step": 13650
    },
    {
      "epoch": 3.506624,
      "grad_norm": 0.12669695913791656,
      "learning_rate": 5.974916815971334e-05,
      "loss": 1.9174,
      "step": 13700
    },
    {
      "epoch": 3.519424,
      "grad_norm": 0.14082123339176178,
      "learning_rate": 5.92372664448426e-05,
      "loss": 1.8606,
      "step": 13750
    },
    {
      "epoch": 3.5322240000000003,
      "grad_norm": 0.12742027640342712,
      "learning_rate": 5.872536472997184e-05,
      "loss": 1.8708,
      "step": 13800
    },
    {
      "epoch": 3.5450239999999997,
      "grad_norm": 0.13561445474624634,
      "learning_rate": 5.82134630151011e-05,
      "loss": 1.8693,
      "step": 13850
    },
    {
      "epoch": 3.557824,
      "grad_norm": 0.12218010425567627,
      "learning_rate": 5.770156130023036e-05,
      "loss": 1.9029,
      "step": 13900
    },
    {
      "epoch": 3.570624,
      "grad_norm": 0.15473847091197968,
      "learning_rate": 5.7189659585359615e-05,
      "loss": 1.8878,
      "step": 13950
    },
    {
      "epoch": 3.583424,
      "grad_norm": 0.12712423503398895,
      "learning_rate": 5.6677757870488865e-05,
      "loss": 1.877,
      "step": 14000
    },
    {
      "epoch": 3.596224,
      "grad_norm": 0.12705819308757782,
      "learning_rate": 5.616585615561812e-05,
      "loss": 1.8877,
      "step": 14050
    },
    {
      "epoch": 3.609024,
      "grad_norm": 0.1260310560464859,
      "learning_rate": 5.565395444074738e-05,
      "loss": 1.8823,
      "step": 14100
    },
    {
      "epoch": 3.621824,
      "grad_norm": 0.12835805118083954,
      "learning_rate": 5.514205272587664e-05,
      "loss": 1.8809,
      "step": 14150
    },
    {
      "epoch": 3.634624,
      "grad_norm": 0.1341070681810379,
      "learning_rate": 5.4630151011005896e-05,
      "loss": 1.8706,
      "step": 14200
    },
    {
      "epoch": 3.647424,
      "grad_norm": 0.13767750561237335,
      "learning_rate": 5.4118249296135146e-05,
      "loss": 1.8849,
      "step": 14250
    },
    {
      "epoch": 3.660224,
      "grad_norm": 0.12361911684274673,
      "learning_rate": 5.3606347581264404e-05,
      "loss": 1.8797,
      "step": 14300
    },
    {
      "epoch": 3.673024,
      "grad_norm": 0.13230270147323608,
      "learning_rate": 5.309444586639365e-05,
      "loss": 1.8851,
      "step": 14350
    },
    {
      "epoch": 3.685824,
      "grad_norm": 0.13192522525787354,
      "learning_rate": 5.2582544151522905e-05,
      "loss": 1.8614,
      "step": 14400
    },
    {
      "epoch": 3.698624,
      "grad_norm": 0.14084984362125397,
      "learning_rate": 5.207064243665216e-05,
      "loss": 1.8952,
      "step": 14450
    },
    {
      "epoch": 3.711424,
      "grad_norm": 0.1300138533115387,
      "learning_rate": 5.155874072178142e-05,
      "loss": 1.8774,
      "step": 14500
    },
    {
      "epoch": 3.724224,
      "grad_norm": 0.12598736584186554,
      "learning_rate": 5.104683900691067e-05,
      "loss": 1.8915,
      "step": 14550
    },
    {
      "epoch": 3.737024,
      "grad_norm": 0.14210891723632812,
      "learning_rate": 5.053493729203993e-05,
      "loss": 1.9084,
      "step": 14600
    },
    {
      "epoch": 3.7498240000000003,
      "grad_norm": 0.14191170036792755,
      "learning_rate": 5.0023035577169186e-05,
      "loss": 1.8845,
      "step": 14650
    },
    {
      "epoch": 3.7626239999999997,
      "grad_norm": 0.12887564301490784,
      "learning_rate": 4.9511133862298444e-05,
      "loss": 1.8984,
      "step": 14700
    },
    {
      "epoch": 3.775424,
      "grad_norm": 0.12993720173835754,
      "learning_rate": 4.89992321474277e-05,
      "loss": 1.8735,
      "step": 14750
    },
    {
      "epoch": 3.788224,
      "grad_norm": 0.1236243024468422,
      "learning_rate": 4.8487330432556946e-05,
      "loss": 1.8865,
      "step": 14800
    },
    {
      "epoch": 3.801024,
      "grad_norm": 0.12799383699893951,
      "learning_rate": 4.79754287176862e-05,
      "loss": 1.875,
      "step": 14850
    },
    {
      "epoch": 3.813824,
      "grad_norm": 0.15240924060344696,
      "learning_rate": 4.746352700281546e-05,
      "loss": 1.8905,
      "step": 14900
    },
    {
      "epoch": 3.826624,
      "grad_norm": 0.1361408829689026,
      "learning_rate": 4.695162528794472e-05,
      "loss": 1.8912,
      "step": 14950
    },
    {
      "epoch": 3.839424,
      "grad_norm": 0.13919390738010406,
      "learning_rate": 4.6439723573073976e-05,
      "loss": 1.8902,
      "step": 15000
    },
    {
      "epoch": 3.852224,
      "grad_norm": 0.12948259711265564,
      "learning_rate": 4.5927821858203226e-05,
      "loss": 1.8668,
      "step": 15050
    },
    {
      "epoch": 3.865024,
      "grad_norm": 0.13883891701698303,
      "learning_rate": 4.541592014333248e-05,
      "loss": 1.8883,
      "step": 15100
    },
    {
      "epoch": 3.877824,
      "grad_norm": 0.12622733414173126,
      "learning_rate": 4.4904018428461735e-05,
      "loss": 1.8783,
      "step": 15150
    },
    {
      "epoch": 3.890624,
      "grad_norm": 0.12639039754867554,
      "learning_rate": 4.439211671359099e-05,
      "loss": 1.8818,
      "step": 15200
    },
    {
      "epoch": 3.9034240000000002,
      "grad_norm": 0.1470654010772705,
      "learning_rate": 4.388021499872025e-05,
      "loss": 1.8913,
      "step": 15250
    },
    {
      "epoch": 3.916224,
      "grad_norm": 0.13453060388565063,
      "learning_rate": 4.336831328384951e-05,
      "loss": 1.8708,
      "step": 15300
    },
    {
      "epoch": 3.929024,
      "grad_norm": 0.12970374524593353,
      "learning_rate": 4.285641156897876e-05,
      "loss": 1.8932,
      "step": 15350
    },
    {
      "epoch": 3.941824,
      "grad_norm": 0.1310272067785263,
      "learning_rate": 4.234450985410801e-05,
      "loss": 1.8861,
      "step": 15400
    },
    {
      "epoch": 3.954624,
      "grad_norm": 0.13356776535511017,
      "learning_rate": 4.1832608139237267e-05,
      "loss": 1.8969,
      "step": 15450
    },
    {
      "epoch": 3.967424,
      "grad_norm": 0.12495853751897812,
      "learning_rate": 4.1320706424366524e-05,
      "loss": 1.8731,
      "step": 15500
    },
    {
      "epoch": 3.9802239999999998,
      "grad_norm": 0.13918054103851318,
      "learning_rate": 4.080880470949578e-05,
      "loss": 1.8765,
      "step": 15550
    },
    {
      "epoch": 3.993024,
      "grad_norm": 0.12906624376773834,
      "learning_rate": 4.029690299462503e-05,
      "loss": 1.8857,
      "step": 15600
    },
    {
      "epoch": 4.005632,
      "grad_norm": 0.13120593130588531,
      "learning_rate": 3.978500127975429e-05,
      "loss": 1.8871,
      "step": 15650
    },
    {
      "epoch": 4.018432,
      "grad_norm": 0.13206638395786285,
      "learning_rate": 3.927309956488354e-05,
      "loss": 1.8865,
      "step": 15700
    },
    {
      "epoch": 4.031232,
      "grad_norm": 0.13550731539726257,
      "learning_rate": 3.87611978500128e-05,
      "loss": 1.8813,
      "step": 15750
    },
    {
      "epoch": 4.044032,
      "grad_norm": 0.12105495482683182,
      "learning_rate": 3.8249296135142056e-05,
      "loss": 1.8765,
      "step": 15800
    },
    {
      "epoch": 4.056832,
      "grad_norm": 0.13891808688640594,
      "learning_rate": 3.773739442027131e-05,
      "loss": 1.8705,
      "step": 15850
    },
    {
      "epoch": 4.069632,
      "grad_norm": 0.13463926315307617,
      "learning_rate": 3.7225492705400564e-05,
      "loss": 1.8652,
      "step": 15900
    },
    {
      "epoch": 4.082432,
      "grad_norm": 0.14048361778259277,
      "learning_rate": 3.671359099052982e-05,
      "loss": 1.8864,
      "step": 15950
    },
    {
      "epoch": 4.095232,
      "grad_norm": 0.12081129848957062,
      "learning_rate": 3.620168927565907e-05,
      "loss": 1.9074,
      "step": 16000
    },
    {
      "epoch": 4.108032,
      "grad_norm": 0.12772014737129211,
      "learning_rate": 3.568978756078833e-05,
      "loss": 1.888,
      "step": 16050
    },
    {
      "epoch": 4.120832,
      "grad_norm": 0.13434721529483795,
      "learning_rate": 3.517788584591759e-05,
      "loss": 1.9001,
      "step": 16100
    },
    {
      "epoch": 4.133632,
      "grad_norm": 0.1328900307416916,
      "learning_rate": 3.466598413104684e-05,
      "loss": 1.8909,
      "step": 16150
    },
    {
      "epoch": 4.146432,
      "grad_norm": 0.14138495922088623,
      "learning_rate": 3.4154082416176096e-05,
      "loss": 1.9064,
      "step": 16200
    },
    {
      "epoch": 4.159232,
      "grad_norm": 0.13497519493103027,
      "learning_rate": 3.3642180701305353e-05,
      "loss": 1.8895,
      "step": 16250
    },
    {
      "epoch": 4.172032,
      "grad_norm": 0.13531066477298737,
      "learning_rate": 3.3130278986434604e-05,
      "loss": 1.8796,
      "step": 16300
    },
    {
      "epoch": 4.184832,
      "grad_norm": 0.13436584174633026,
      "learning_rate": 3.261837727156386e-05,
      "loss": 1.8727,
      "step": 16350
    },
    {
      "epoch": 4.197632,
      "grad_norm": 0.14240662753582,
      "learning_rate": 3.210647555669312e-05,
      "loss": 1.8582,
      "step": 16400
    },
    {
      "epoch": 4.210432,
      "grad_norm": 0.13277478516101837,
      "learning_rate": 3.159457384182237e-05,
      "loss": 1.8938,
      "step": 16450
    },
    {
      "epoch": 4.223232,
      "grad_norm": 0.12904514372348785,
      "learning_rate": 3.108267212695163e-05,
      "loss": 1.8876,
      "step": 16500
    },
    {
      "epoch": 4.236032,
      "grad_norm": 0.13320392370224,
      "learning_rate": 3.0570770412080885e-05,
      "loss": 1.8945,
      "step": 16550
    },
    {
      "epoch": 4.248832,
      "grad_norm": 0.13615448772907257,
      "learning_rate": 3.0058868697210136e-05,
      "loss": 1.889,
      "step": 16600
    },
    {
      "epoch": 4.261632,
      "grad_norm": 0.12233603000640869,
      "learning_rate": 2.954696698233939e-05,
      "loss": 1.8906,
      "step": 16650
    },
    {
      "epoch": 4.274432,
      "grad_norm": 0.13187988102436066,
      "learning_rate": 2.9035065267468648e-05,
      "loss": 1.8699,
      "step": 16700
    },
    {
      "epoch": 4.287232,
      "grad_norm": 0.12593521177768707,
      "learning_rate": 2.8523163552597905e-05,
      "loss": 1.8716,
      "step": 16750
    },
    {
      "epoch": 4.300032,
      "grad_norm": 0.12907619774341583,
      "learning_rate": 2.801126183772716e-05,
      "loss": 1.8817,
      "step": 16800
    },
    {
      "epoch": 4.312832,
      "grad_norm": 0.14053763449192047,
      "learning_rate": 2.749936012285641e-05,
      "loss": 1.8645,
      "step": 16850
    },
    {
      "epoch": 4.325632,
      "grad_norm": 0.14444735646247864,
      "learning_rate": 2.6987458407985668e-05,
      "loss": 1.8868,
      "step": 16900
    },
    {
      "epoch": 4.338432,
      "grad_norm": 0.15068483352661133,
      "learning_rate": 2.6475556693114922e-05,
      "loss": 1.8976,
      "step": 16950
    },
    {
      "epoch": 4.3512319999999995,
      "grad_norm": 0.1259642392396927,
      "learning_rate": 2.596365497824418e-05,
      "loss": 1.8855,
      "step": 17000
    },
    {
      "epoch": 4.364032,
      "grad_norm": 0.12086387723684311,
      "learning_rate": 2.5451753263373433e-05,
      "loss": 1.8775,
      "step": 17050
    },
    {
      "epoch": 4.376832,
      "grad_norm": 0.12713927030563354,
      "learning_rate": 2.4939851548502688e-05,
      "loss": 1.9035,
      "step": 17100
    },
    {
      "epoch": 4.389632,
      "grad_norm": 0.13516685366630554,
      "learning_rate": 2.4427949833631945e-05,
      "loss": 1.8626,
      "step": 17150
    },
    {
      "epoch": 4.402432,
      "grad_norm": 0.13704393804073334,
      "learning_rate": 2.39160481187612e-05,
      "loss": 1.882,
      "step": 17200
    },
    {
      "epoch": 4.415232,
      "grad_norm": 0.12977226078510284,
      "learning_rate": 2.3404146403890454e-05,
      "loss": 1.8779,
      "step": 17250
    },
    {
      "epoch": 4.428032,
      "grad_norm": 0.1391625553369522,
      "learning_rate": 2.289224468901971e-05,
      "loss": 1.8925,
      "step": 17300
    },
    {
      "epoch": 4.440832,
      "grad_norm": 0.12753735482692719,
      "learning_rate": 2.2380342974148965e-05,
      "loss": 1.8739,
      "step": 17350
    },
    {
      "epoch": 4.453632,
      "grad_norm": 0.1328820437192917,
      "learning_rate": 2.186844125927822e-05,
      "loss": 1.8731,
      "step": 17400
    },
    {
      "epoch": 4.466432,
      "grad_norm": 0.14060384035110474,
      "learning_rate": 2.1356539544407474e-05,
      "loss": 1.8658,
      "step": 17450
    },
    {
      "epoch": 4.479232,
      "grad_norm": 0.13325656950473785,
      "learning_rate": 2.084463782953673e-05,
      "loss": 1.889,
      "step": 17500
    },
    {
      "epoch": 4.492032,
      "grad_norm": 0.13103948533535004,
      "learning_rate": 2.0332736114665985e-05,
      "loss": 1.8864,
      "step": 17550
    },
    {
      "epoch": 4.504832,
      "grad_norm": 0.1314498335123062,
      "learning_rate": 1.982083439979524e-05,
      "loss": 1.8931,
      "step": 17600
    },
    {
      "epoch": 4.517632,
      "grad_norm": 0.14317813515663147,
      "learning_rate": 1.9308932684924497e-05,
      "loss": 1.8733,
      "step": 17650
    },
    {
      "epoch": 4.530432,
      "grad_norm": 0.13511471450328827,
      "learning_rate": 1.879703097005375e-05,
      "loss": 1.8721,
      "step": 17700
    },
    {
      "epoch": 4.543232,
      "grad_norm": 0.12343425303697586,
      "learning_rate": 1.8285129255183005e-05,
      "loss": 1.8488,
      "step": 17750
    },
    {
      "epoch": 4.556032,
      "grad_norm": 0.15031005442142487,
      "learning_rate": 1.7773227540312263e-05,
      "loss": 1.8881,
      "step": 17800
    },
    {
      "epoch": 4.5688320000000004,
      "grad_norm": 0.14238803088665009,
      "learning_rate": 1.7261325825441517e-05,
      "loss": 1.8871,
      "step": 17850
    },
    {
      "epoch": 4.581632,
      "grad_norm": 0.12749017775058746,
      "learning_rate": 1.674942411057077e-05,
      "loss": 1.8773,
      "step": 17900
    },
    {
      "epoch": 4.594432,
      "grad_norm": 0.15203019976615906,
      "learning_rate": 1.623752239570003e-05,
      "loss": 1.882,
      "step": 17950
    },
    {
      "epoch": 4.607232,
      "grad_norm": 0.13807259500026703,
      "learning_rate": 1.572562068082928e-05,
      "loss": 1.8644,
      "step": 18000
    },
    {
      "epoch": 4.620032,
      "grad_norm": 0.13042746484279633,
      "learning_rate": 1.5213718965958537e-05,
      "loss": 1.8901,
      "step": 18050
    },
    {
      "epoch": 4.632832,
      "grad_norm": 0.13835979998111725,
      "learning_rate": 1.4701817251087791e-05,
      "loss": 1.8596,
      "step": 18100
    },
    {
      "epoch": 4.645632,
      "grad_norm": 0.13359731435775757,
      "learning_rate": 1.4189915536217047e-05,
      "loss": 1.9029,
      "step": 18150
    },
    {
      "epoch": 4.658432,
      "grad_norm": 0.12498590350151062,
      "learning_rate": 1.3678013821346303e-05,
      "loss": 1.9006,
      "step": 18200
    },
    {
      "epoch": 4.671232,
      "grad_norm": 0.13956119120121002,
      "learning_rate": 1.3166112106475555e-05,
      "loss": 1.8948,
      "step": 18250
    },
    {
      "epoch": 4.684032,
      "grad_norm": 0.14051266014575958,
      "learning_rate": 1.2654210391604813e-05,
      "loss": 1.8948,
      "step": 18300
    },
    {
      "epoch": 4.696832,
      "grad_norm": 0.12306266278028488,
      "learning_rate": 1.2142308676734069e-05,
      "loss": 1.8856,
      "step": 18350
    },
    {
      "epoch": 4.709632,
      "grad_norm": 0.1445285677909851,
      "learning_rate": 1.1630406961863323e-05,
      "loss": 1.8829,
      "step": 18400
    },
    {
      "epoch": 4.7224319999999995,
      "grad_norm": 0.12204260379076004,
      "learning_rate": 1.1118505246992577e-05,
      "loss": 1.8886,
      "step": 18450
    },
    {
      "epoch": 4.735232,
      "grad_norm": 0.13890889286994934,
      "learning_rate": 1.0606603532121835e-05,
      "loss": 1.8774,
      "step": 18500
    },
    {
      "epoch": 4.748032,
      "grad_norm": 0.12565773725509644,
      "learning_rate": 1.0094701817251089e-05,
      "loss": 1.8769,
      "step": 18550
    },
    {
      "epoch": 4.760832,
      "grad_norm": 0.1355678290128708,
      "learning_rate": 9.582800102380343e-06,
      "loss": 1.8721,
      "step": 18600
    },
    {
      "epoch": 4.773632,
      "grad_norm": 0.13540765643119812,
      "learning_rate": 9.070898387509599e-06,
      "loss": 1.8754,
      "step": 18650
    },
    {
      "epoch": 4.786432,
      "grad_norm": 0.1374519020318985,
      "learning_rate": 8.558996672638855e-06,
      "loss": 1.8956,
      "step": 18700
    },
    {
      "epoch": 4.799232,
      "grad_norm": 0.14353588223457336,
      "learning_rate": 8.047094957768109e-06,
      "loss": 1.8675,
      "step": 18750
    },
    {
      "epoch": 4.812032,
      "grad_norm": 0.1364404410123825,
      "learning_rate": 7.535193242897364e-06,
      "loss": 1.8919,
      "step": 18800
    },
    {
      "epoch": 4.824832,
      "grad_norm": 0.13763870298862457,
      "learning_rate": 7.02329152802662e-06,
      "loss": 1.8784,
      "step": 18850
    },
    {
      "epoch": 4.837632,
      "grad_norm": 0.12612897157669067,
      "learning_rate": 6.511389813155875e-06,
      "loss": 1.8887,
      "step": 18900
    },
    {
      "epoch": 4.850432,
      "grad_norm": 0.13165396451950073,
      "learning_rate": 5.99948809828513e-06,
      "loss": 1.8745,
      "step": 18950
    },
    {
      "epoch": 4.863232,
      "grad_norm": 0.15020187199115753,
      "learning_rate": 5.487586383414385e-06,
      "loss": 1.89,
      "step": 19000
    },
    {
      "epoch": 4.876032,
      "grad_norm": 0.1265646070241928,
      "learning_rate": 4.97568466854364e-06,
      "loss": 1.9102,
      "step": 19050
    },
    {
      "epoch": 4.888832,
      "grad_norm": 0.14230822026729584,
      "learning_rate": 4.4637829536728955e-06,
      "loss": 1.8869,
      "step": 19100
    },
    {
      "epoch": 4.901632,
      "grad_norm": 0.13486027717590332,
      "learning_rate": 3.95188123880215e-06,
      "loss": 1.891,
      "step": 19150
    },
    {
      "epoch": 4.914432,
      "grad_norm": 0.1280142366886139,
      "learning_rate": 3.4399795239314055e-06,
      "loss": 1.8785,
      "step": 19200
    },
    {
      "epoch": 4.927232,
      "grad_norm": 0.1326802670955658,
      "learning_rate": 2.9280778090606605e-06,
      "loss": 1.8865,
      "step": 19250
    },
    {
      "epoch": 4.940032,
      "grad_norm": 0.14142389595508575,
      "learning_rate": 2.416176094189916e-06,
      "loss": 1.8708,
      "step": 19300
    },
    {
      "epoch": 4.952832,
      "grad_norm": 0.14313381910324097,
      "learning_rate": 1.904274379319171e-06,
      "loss": 1.8735,
      "step": 19350
    },
    {
      "epoch": 4.965632,
      "grad_norm": 0.15136772394180298,
      "learning_rate": 1.392372664448426e-06,
      "loss": 1.8871,
      "step": 19400
    },
    {
      "epoch": 4.978432,
      "grad_norm": 0.14840297400951385,
      "learning_rate": 8.804709495776812e-07,
      "loss": 1.8707,
      "step": 19450
    },
    {
      "epoch": 4.991232,
      "grad_norm": 0.16020706295967102,
      "learning_rate": 3.685692347069363e-07,
      "loss": 1.88,
      "step": 19500
    }
  ],
  "logging_steps": 50,
  "max_steps": 19535,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 5000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.32512008916566e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
