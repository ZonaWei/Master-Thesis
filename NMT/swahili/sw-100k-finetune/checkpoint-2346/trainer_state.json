{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 100,
  "global_step": 2346,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0256,
      "grad_norm": 0.15277767181396484,
      "learning_rate": 0.00019838022165387896,
      "loss": 2.5509,
      "step": 20
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.11470481008291245,
      "learning_rate": 0.0001966751918158568,
      "loss": 2.3155,
      "step": 40
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.12195952981710434,
      "learning_rate": 0.0001949701619778346,
      "loss": 2.1662,
      "step": 60
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.09722480922937393,
      "learning_rate": 0.00019326513213981247,
      "loss": 2.0866,
      "step": 80
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.08553026616573334,
      "learning_rate": 0.0001915601023017903,
      "loss": 2.0267,
      "step": 100
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.08881253749132156,
      "learning_rate": 0.00018985507246376812,
      "loss": 2.0408,
      "step": 120
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.09967876970767975,
      "learning_rate": 0.00018815004262574597,
      "loss": 2.0359,
      "step": 140
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.0911295935511589,
      "learning_rate": 0.0001864450127877238,
      "loss": 2.0182,
      "step": 160
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.10325626283884048,
      "learning_rate": 0.00018473998294970162,
      "loss": 2.022,
      "step": 180
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.09365866333246231,
      "learning_rate": 0.00018303495311167947,
      "loss": 2.0235,
      "step": 200
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.10364009439945221,
      "learning_rate": 0.0001813299232736573,
      "loss": 2.0349,
      "step": 220
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.09166654199361801,
      "learning_rate": 0.00017962489343563513,
      "loss": 2.0293,
      "step": 240
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.09881893545389175,
      "learning_rate": 0.00017791986359761298,
      "loss": 2.0122,
      "step": 260
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.09262742102146149,
      "learning_rate": 0.0001762148337595908,
      "loss": 2.0129,
      "step": 280
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.09986936300992966,
      "learning_rate": 0.00017450980392156863,
      "loss": 2.0229,
      "step": 300
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.09353259950876236,
      "learning_rate": 0.00017280477408354648,
      "loss": 2.0171,
      "step": 320
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.09405524283647537,
      "learning_rate": 0.0001710997442455243,
      "loss": 2.0333,
      "step": 340
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.10599157214164734,
      "learning_rate": 0.00016939471440750213,
      "loss": 2.005,
      "step": 360
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.11018238961696625,
      "learning_rate": 0.00016768968456948,
      "loss": 2.0268,
      "step": 380
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.10370350629091263,
      "learning_rate": 0.0001659846547314578,
      "loss": 1.992,
      "step": 400
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.10998877137899399,
      "learning_rate": 0.00016427962489343564,
      "loss": 1.9989,
      "step": 420
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.10051696002483368,
      "learning_rate": 0.0001625745950554135,
      "loss": 2.0006,
      "step": 440
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.10016768425703049,
      "learning_rate": 0.00016086956521739132,
      "loss": 2.0011,
      "step": 460
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.10766275972127914,
      "learning_rate": 0.00015916453537936914,
      "loss": 2.0472,
      "step": 480
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.11046485602855682,
      "learning_rate": 0.000157459505541347,
      "loss": 1.9771,
      "step": 500
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.11066874116659164,
      "learning_rate": 0.00015575447570332482,
      "loss": 1.99,
      "step": 520
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.09647637605667114,
      "learning_rate": 0.00015404944586530265,
      "loss": 2.0226,
      "step": 540
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.1103745624423027,
      "learning_rate": 0.0001523444160272805,
      "loss": 2.0295,
      "step": 560
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.10434693098068237,
      "learning_rate": 0.00015063938618925833,
      "loss": 1.9729,
      "step": 580
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.10333311557769775,
      "learning_rate": 0.00014893435635123615,
      "loss": 1.9338,
      "step": 600
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.1024404764175415,
      "learning_rate": 0.000147229326513214,
      "loss": 2.0196,
      "step": 620
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.109444759786129,
      "learning_rate": 0.00014552429667519183,
      "loss": 2.0316,
      "step": 640
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.10708902776241302,
      "learning_rate": 0.00014381926683716966,
      "loss": 2.007,
      "step": 660
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.10831708461046219,
      "learning_rate": 0.0001421142369991475,
      "loss": 2.0301,
      "step": 680
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.10084708780050278,
      "learning_rate": 0.00014040920716112533,
      "loss": 1.9668,
      "step": 700
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.10338794440031052,
      "learning_rate": 0.00013870417732310316,
      "loss": 2.0299,
      "step": 720
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.1090022623538971,
      "learning_rate": 0.000136999147485081,
      "loss": 1.9719,
      "step": 740
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.11993961036205292,
      "learning_rate": 0.00013529411764705884,
      "loss": 1.9651,
      "step": 760
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.10864448547363281,
      "learning_rate": 0.00013358908780903666,
      "loss": 2.0058,
      "step": 780
    },
    {
      "epoch": 1.02304,
      "grad_norm": 0.1108010783791542,
      "learning_rate": 0.00013188405797101452,
      "loss": 2.0065,
      "step": 800
    },
    {
      "epoch": 1.04864,
      "grad_norm": 0.10677988827228546,
      "learning_rate": 0.00013017902813299234,
      "loss": 1.986,
      "step": 820
    },
    {
      "epoch": 1.07424,
      "grad_norm": 0.11350227892398834,
      "learning_rate": 0.00012847399829497017,
      "loss": 2.0146,
      "step": 840
    },
    {
      "epoch": 1.09984,
      "grad_norm": 0.10715976357460022,
      "learning_rate": 0.00012676896845694802,
      "loss": 2.0301,
      "step": 860
    },
    {
      "epoch": 1.12544,
      "grad_norm": 0.11511362344026566,
      "learning_rate": 0.00012506393861892585,
      "loss": 1.9685,
      "step": 880
    },
    {
      "epoch": 1.15104,
      "grad_norm": 0.10415441542863846,
      "learning_rate": 0.00012335890878090367,
      "loss": 1.9714,
      "step": 900
    },
    {
      "epoch": 1.17664,
      "grad_norm": 0.11032113432884216,
      "learning_rate": 0.00012165387894288151,
      "loss": 1.9962,
      "step": 920
    },
    {
      "epoch": 1.20224,
      "grad_norm": 0.11947034299373627,
      "learning_rate": 0.00011994884910485935,
      "loss": 1.971,
      "step": 940
    },
    {
      "epoch": 1.22784,
      "grad_norm": 0.09888410568237305,
      "learning_rate": 0.00011824381926683719,
      "loss": 2.0065,
      "step": 960
    },
    {
      "epoch": 1.2534399999999999,
      "grad_norm": 0.10704341530799866,
      "learning_rate": 0.00011653878942881502,
      "loss": 2.0045,
      "step": 980
    },
    {
      "epoch": 1.27904,
      "grad_norm": 0.09856797754764557,
      "learning_rate": 0.00011483375959079286,
      "loss": 1.9988,
      "step": 1000
    },
    {
      "epoch": 1.30464,
      "grad_norm": 0.10771676152944565,
      "learning_rate": 0.00011312872975277067,
      "loss": 1.9662,
      "step": 1020
    },
    {
      "epoch": 1.3302399999999999,
      "grad_norm": 0.11416862905025482,
      "learning_rate": 0.00011142369991474851,
      "loss": 2.0047,
      "step": 1040
    },
    {
      "epoch": 1.35584,
      "grad_norm": 0.1124705821275711,
      "learning_rate": 0.00010971867007672633,
      "loss": 1.9729,
      "step": 1060
    },
    {
      "epoch": 1.38144,
      "grad_norm": 0.10669618099927902,
      "learning_rate": 0.00010801364023870417,
      "loss": 1.9904,
      "step": 1080
    },
    {
      "epoch": 1.40704,
      "grad_norm": 0.12106936424970627,
      "learning_rate": 0.00010630861040068201,
      "loss": 1.9937,
      "step": 1100
    },
    {
      "epoch": 1.4326400000000001,
      "grad_norm": 0.11541230976581573,
      "learning_rate": 0.00010460358056265984,
      "loss": 1.9766,
      "step": 1120
    },
    {
      "epoch": 1.45824,
      "grad_norm": 0.11115624755620956,
      "learning_rate": 0.00010289855072463768,
      "loss": 1.9653,
      "step": 1140
    },
    {
      "epoch": 1.48384,
      "grad_norm": 0.11121442168951035,
      "learning_rate": 0.00010119352088661552,
      "loss": 1.9886,
      "step": 1160
    },
    {
      "epoch": 1.5094400000000001,
      "grad_norm": 0.11059421300888062,
      "learning_rate": 9.948849104859336e-05,
      "loss": 1.9861,
      "step": 1180
    },
    {
      "epoch": 1.53504,
      "grad_norm": 0.11261526495218277,
      "learning_rate": 9.77834612105712e-05,
      "loss": 1.9936,
      "step": 1200
    },
    {
      "epoch": 1.56064,
      "grad_norm": 0.10785959661006927,
      "learning_rate": 9.607843137254903e-05,
      "loss": 1.9951,
      "step": 1220
    },
    {
      "epoch": 1.58624,
      "grad_norm": 0.11096260696649551,
      "learning_rate": 9.437340153452686e-05,
      "loss": 2.0073,
      "step": 1240
    },
    {
      "epoch": 1.61184,
      "grad_norm": 0.11317937821149826,
      "learning_rate": 9.26683716965047e-05,
      "loss": 2.0189,
      "step": 1260
    },
    {
      "epoch": 1.63744,
      "grad_norm": 0.11716274917125702,
      "learning_rate": 9.096334185848254e-05,
      "loss": 2.018,
      "step": 1280
    },
    {
      "epoch": 1.66304,
      "grad_norm": 0.11374300718307495,
      "learning_rate": 8.925831202046036e-05,
      "loss": 2.016,
      "step": 1300
    },
    {
      "epoch": 1.68864,
      "grad_norm": 0.11408010870218277,
      "learning_rate": 8.75532821824382e-05,
      "loss": 2.0097,
      "step": 1320
    },
    {
      "epoch": 1.71424,
      "grad_norm": 0.1055072546005249,
      "learning_rate": 8.584825234441604e-05,
      "loss": 1.9864,
      "step": 1340
    },
    {
      "epoch": 1.73984,
      "grad_norm": 0.11757084727287292,
      "learning_rate": 8.414322250639387e-05,
      "loss": 1.9808,
      "step": 1360
    },
    {
      "epoch": 1.76544,
      "grad_norm": 0.1150825172662735,
      "learning_rate": 8.243819266837171e-05,
      "loss": 1.966,
      "step": 1380
    },
    {
      "epoch": 1.79104,
      "grad_norm": 0.11413172632455826,
      "learning_rate": 8.073316283034955e-05,
      "loss": 1.9479,
      "step": 1400
    },
    {
      "epoch": 1.81664,
      "grad_norm": 0.10971717536449432,
      "learning_rate": 7.902813299232737e-05,
      "loss": 1.9661,
      "step": 1420
    },
    {
      "epoch": 1.8422399999999999,
      "grad_norm": 0.12156393378973007,
      "learning_rate": 7.73231031543052e-05,
      "loss": 2.0099,
      "step": 1440
    },
    {
      "epoch": 1.86784,
      "grad_norm": 0.11728519201278687,
      "learning_rate": 7.561807331628304e-05,
      "loss": 2.0066,
      "step": 1460
    },
    {
      "epoch": 1.89344,
      "grad_norm": 0.1233518123626709,
      "learning_rate": 7.391304347826086e-05,
      "loss": 1.979,
      "step": 1480
    },
    {
      "epoch": 1.9190399999999999,
      "grad_norm": 0.12366356700658798,
      "learning_rate": 7.22080136402387e-05,
      "loss": 1.9787,
      "step": 1500
    },
    {
      "epoch": 1.9446400000000001,
      "grad_norm": 0.12527702748775482,
      "learning_rate": 7.050298380221654e-05,
      "loss": 1.9685,
      "step": 1520
    },
    {
      "epoch": 1.97024,
      "grad_norm": 0.12405496090650558,
      "learning_rate": 6.879795396419437e-05,
      "loss": 1.9991,
      "step": 1540
    },
    {
      "epoch": 1.9958399999999998,
      "grad_norm": 0.14010977745056152,
      "learning_rate": 6.70929241261722e-05,
      "loss": 1.9406,
      "step": 1560
    },
    {
      "epoch": 2.02048,
      "grad_norm": 0.10742837190628052,
      "learning_rate": 6.538789428815005e-05,
      "loss": 1.9854,
      "step": 1580
    },
    {
      "epoch": 2.04608,
      "grad_norm": 0.12726877629756927,
      "learning_rate": 6.368286445012787e-05,
      "loss": 1.9688,
      "step": 1600
    },
    {
      "epoch": 2.07168,
      "grad_norm": 0.11459796130657196,
      "learning_rate": 6.197783461210571e-05,
      "loss": 1.9652,
      "step": 1620
    },
    {
      "epoch": 2.09728,
      "grad_norm": 0.11470924317836761,
      "learning_rate": 6.0272804774083543e-05,
      "loss": 1.99,
      "step": 1640
    },
    {
      "epoch": 2.12288,
      "grad_norm": 0.1276801973581314,
      "learning_rate": 5.856777493606138e-05,
      "loss": 2.0021,
      "step": 1660
    },
    {
      "epoch": 2.14848,
      "grad_norm": 0.12049276381731033,
      "learning_rate": 5.6862745098039215e-05,
      "loss": 1.9928,
      "step": 1680
    },
    {
      "epoch": 2.17408,
      "grad_norm": 0.12315284460783005,
      "learning_rate": 5.515771526001705e-05,
      "loss": 1.9609,
      "step": 1700
    },
    {
      "epoch": 2.19968,
      "grad_norm": 0.11627322435379028,
      "learning_rate": 5.345268542199489e-05,
      "loss": 1.9993,
      "step": 1720
    },
    {
      "epoch": 2.22528,
      "grad_norm": 0.11893601715564728,
      "learning_rate": 5.174765558397272e-05,
      "loss": 1.9924,
      "step": 1740
    },
    {
      "epoch": 2.25088,
      "grad_norm": 0.11189893633127213,
      "learning_rate": 5.004262574595056e-05,
      "loss": 1.9683,
      "step": 1760
    },
    {
      "epoch": 2.27648,
      "grad_norm": 0.12192364782094955,
      "learning_rate": 4.833759590792839e-05,
      "loss": 1.9806,
      "step": 1780
    },
    {
      "epoch": 2.30208,
      "grad_norm": 0.1232464611530304,
      "learning_rate": 4.6632566069906224e-05,
      "loss": 1.9656,
      "step": 1800
    },
    {
      "epoch": 2.32768,
      "grad_norm": 0.13022750616073608,
      "learning_rate": 4.492753623188406e-05,
      "loss": 1.9983,
      "step": 1820
    },
    {
      "epoch": 2.35328,
      "grad_norm": 0.11987343430519104,
      "learning_rate": 4.3222506393861896e-05,
      "loss": 1.9694,
      "step": 1840
    },
    {
      "epoch": 2.37888,
      "grad_norm": 0.11385944485664368,
      "learning_rate": 4.151747655583973e-05,
      "loss": 1.9817,
      "step": 1860
    },
    {
      "epoch": 2.40448,
      "grad_norm": 0.11915283650159836,
      "learning_rate": 3.981244671781757e-05,
      "loss": 2.0284,
      "step": 1880
    },
    {
      "epoch": 2.4300800000000002,
      "grad_norm": 0.12099533528089523,
      "learning_rate": 3.81074168797954e-05,
      "loss": 1.9957,
      "step": 1900
    },
    {
      "epoch": 2.45568,
      "grad_norm": 0.11509185284376144,
      "learning_rate": 3.640238704177323e-05,
      "loss": 1.9703,
      "step": 1920
    },
    {
      "epoch": 2.48128,
      "grad_norm": 0.12800335884094238,
      "learning_rate": 3.469735720375107e-05,
      "loss": 1.9743,
      "step": 1940
    },
    {
      "epoch": 2.5068799999999998,
      "grad_norm": 0.12293820083141327,
      "learning_rate": 3.2992327365728904e-05,
      "loss": 1.9453,
      "step": 1960
    },
    {
      "epoch": 2.53248,
      "grad_norm": 0.12122783064842224,
      "learning_rate": 3.128729752770674e-05,
      "loss": 2.0067,
      "step": 1980
    },
    {
      "epoch": 2.55808,
      "grad_norm": 0.11732035875320435,
      "learning_rate": 2.9582267689684573e-05,
      "loss": 1.9794,
      "step": 2000
    },
    {
      "epoch": 2.58368,
      "grad_norm": 0.13287371397018433,
      "learning_rate": 2.787723785166241e-05,
      "loss": 1.9632,
      "step": 2020
    },
    {
      "epoch": 2.60928,
      "grad_norm": 0.12627077102661133,
      "learning_rate": 2.617220801364024e-05,
      "loss": 1.9763,
      "step": 2040
    },
    {
      "epoch": 2.63488,
      "grad_norm": 0.11379358172416687,
      "learning_rate": 2.4467178175618073e-05,
      "loss": 2.006,
      "step": 2060
    },
    {
      "epoch": 2.6604799999999997,
      "grad_norm": 0.12452936917543411,
      "learning_rate": 2.276214833759591e-05,
      "loss": 1.9565,
      "step": 2080
    },
    {
      "epoch": 2.68608,
      "grad_norm": 0.1262306421995163,
      "learning_rate": 2.1057118499573745e-05,
      "loss": 1.9679,
      "step": 2100
    },
    {
      "epoch": 2.71168,
      "grad_norm": 0.12942899763584137,
      "learning_rate": 1.9352088661551578e-05,
      "loss": 1.9745,
      "step": 2120
    },
    {
      "epoch": 2.73728,
      "grad_norm": 0.1213705912232399,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 1.9805,
      "step": 2140
    },
    {
      "epoch": 2.76288,
      "grad_norm": 0.12452559173107147,
      "learning_rate": 1.5942028985507246e-05,
      "loss": 2.025,
      "step": 2160
    },
    {
      "epoch": 2.78848,
      "grad_norm": 0.12894301116466522,
      "learning_rate": 1.423699914748508e-05,
      "loss": 2.0004,
      "step": 2180
    },
    {
      "epoch": 2.81408,
      "grad_norm": 0.13037040829658508,
      "learning_rate": 1.2531969309462916e-05,
      "loss": 1.9712,
      "step": 2200
    },
    {
      "epoch": 2.83968,
      "grad_norm": 0.12969540059566498,
      "learning_rate": 1.082693947144075e-05,
      "loss": 1.9736,
      "step": 2220
    },
    {
      "epoch": 2.8652800000000003,
      "grad_norm": 0.13057005405426025,
      "learning_rate": 9.121909633418585e-06,
      "loss": 1.978,
      "step": 2240
    },
    {
      "epoch": 2.89088,
      "grad_norm": 0.11816145479679108,
      "learning_rate": 7.41687979539642e-06,
      "loss": 1.9611,
      "step": 2260
    },
    {
      "epoch": 2.91648,
      "grad_norm": 0.1259518712759018,
      "learning_rate": 5.711849957374255e-06,
      "loss": 1.9957,
      "step": 2280
    },
    {
      "epoch": 2.94208,
      "grad_norm": 0.13880246877670288,
      "learning_rate": 4.006820119352089e-06,
      "loss": 2.0049,
      "step": 2300
    },
    {
      "epoch": 2.96768,
      "grad_norm": 0.12062641233205795,
      "learning_rate": 2.3017902813299235e-06,
      "loss": 1.9751,
      "step": 2320
    },
    {
      "epoch": 2.99328,
      "grad_norm": 0.11912485957145691,
      "learning_rate": 5.967604433077579e-07,
      "loss": 2.0053,
      "step": 2340
    }
  ],
  "logging_steps": 20,
  "max_steps": 2346,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.751645409738752e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
